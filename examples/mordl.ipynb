{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading UniversalDependencies/UD_Russian-SynTagRus contents\n",
      "done: 9525 bytes\n"
     ]
    }
   ],
   "source": [
    "from corpuscula.corpus_utils import download_ud, UniversalDependencies, \\\n",
    "                                    AdjustedForSpeech\n",
    "import junky\n",
    "from mordl import UposTagger\n",
    "\n",
    "BERT_MODEL_FN = 'bert-base-multilingual-cased'\n",
    "MODEL_FN = 'upos_model'\n",
    "SEED=42\n",
    "\n",
    "# we use UD Taiga corpus only as example. For real model training comment\n",
    "# Taiga and uncomment SynTagRus\n",
    "#corpus_name = 'UD_Russian-Taiga'\n",
    "corpus_name = 'UD_Russian-SynTagRus'\n",
    "download_ud(corpus_name, overwrite=False)\n",
    "corpus = UniversalDependencies(corpus_name)\n",
    "#corpus = AdjustedForSpeech(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Load corpus\n",
      "[=================================================] 48814           \n",
      "Corpus has been loaded: 48814 sentences, 871526 tokens\n",
      "Test: Load corpus\n",
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UPOS TAGGER TRAINING PIPELINE ===\n",
      "\n",
      "BERT MODEL TUNING 'bert-base-multilingual-cased'. The result's model name will be 'upos_bert-base-multilingual-cased_len512_ep3_bat8_seed42'\n",
      "Loading tokenizer...\n",
      "Tokenizer is loaded. Vocab size: 119547\n",
      "Corpora processing... done.\n",
      "Loading model 'bert-base-multilingual-cased'... done.\n",
      "Epoch 1: 100%|██████████| 48814/48814 [12:32<00:00, 64.86it/s, train_loss=0.0226] \n",
      "Average train loss: 0.08670417480997196\n",
      "Dev: accuracy = 0.98225193\n",
      "Dev: precision = 0.96434810\n",
      "Dev: recall = 0.88591372\n",
      "Dev: f1_score = 0.89677961\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to upos_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "Epoch 2: 100%|██████████| 48814/48814 [12:24<00:00, 65.54it/s, train_loss=0.00107] \n",
      "Average train loss: 0.03311146485173351\n",
      "Dev: accuracy = 0.98572055\n",
      "Dev: precision = 0.96505439\n",
      "Dev: recall = 0.93086512\n",
      "Dev: f1_score = 0.94336020\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to upos_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "Epoch 3: 100%|██████████| 48814/48814 [12:18<00:00, 66.13it/s, train_loss=0.0202]  \n",
      "Average train loss: 0.016803036195094795\n",
      "Dev: accuracy = 0.98741197\n",
      "Dev: precision = 0.95841448\n",
      "Dev: recall = 0.93435396\n",
      "Dev: f1_score = 0.94485568\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to upos_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "\n",
      "DATASETS CREATION\n",
      "Tokenizing\n",
      "100%|██████████| 48814/48814 [00:33<00:00, 1475.02it/s]\n",
      "Vectorizing\n",
      "100%|██████████| 763/763 [01:15<00:00, 10.05it/s]\n",
      "Adjusting\n",
      "100%|██████████| 48814/48814 [02:51<00:00, 284.54it/s]\n",
      "Vectorizing\n",
      "100%|██████████| 103/103 [00:10<00:00,  9.70it/s]\n",
      "Adjusting\n",
      "100%|██████████| 6584/6584 [00:23<00:00, 284.60it/s]\n",
      "\n",
      "MODEL CREATION\n",
      "Config saved\n",
      "\n",
      "MODEL TRAINING\n",
      "Epoch 1: 100%|██████████| 48814/48814 [04:17<00:00, 189.74it/s, train_loss=0.00509]\n",
      "Epoch 1: \n",
      "Losses: train = 0.01626948, test = 0.01995598\n",
      "Test: accuracy = 0.98956856\n",
      "Test: precision = 0.95496492\n",
      "Test: recall = 0.93318156\n",
      "Test: f1_score = 0.94246525\n",
      "new maximum score 0.98956856\n",
      "Saving state_dict... done.\n",
      "Epoch 2: 100%|██████████| 48814/48814 [04:17<00:00, 189.46it/s, train_loss=0.00405]\n",
      "Epoch 2: \n",
      "Losses: train = 0.00425636, test = 0.01898148\n",
      "Test: accuracy = 0.98965296\n",
      "Test: precision = 0.95361310\n",
      "Test: recall = 0.94107904\n",
      "Test: f1_score = 0.94691349\n",
      "new maximum score 0.98965296\n",
      "Saving state_dict... done.\n",
      "Epoch 3: 100%|██████████| 48814/48814 [04:18<00:00, 189.06it/s, train_loss=0.00374]\n",
      "Epoch 3: \n",
      "Losses: train = 0.00392976, test = 0.01865952\n",
      "Test: accuracy = 0.98958544\n",
      "Test: precision = 0.95517046\n",
      "Test: recall = 0.93785100\n",
      "Test: f1_score = 0.94596551\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 4: 100%|██████████| 48814/48814 [04:18<00:00, 188.92it/s, train_loss=0.00352]\n",
      "Epoch 4: \n",
      "Losses: train = 0.00367688, test = 0.01823294\n",
      "Test: accuracy = 0.98944197\n",
      "Test: precision = 0.95090729\n",
      "Test: recall = 0.93884456\n",
      "Test: f1_score = 0.94460519\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 5: 100%|██████████| 48814/48814 [04:18<00:00, 188.84it/s, train_loss=0.00362]\n",
      "Epoch 5: \n",
      "Losses: train = 0.00357859, test = 0.01808777\n",
      "Test: accuracy = 0.98979644\n",
      "Test: precision = 0.95461448\n",
      "Test: recall = 0.94040067\n",
      "Test: f1_score = 0.94703046\n",
      "new maximum score 0.98979644\n",
      "Saving state_dict... done.\n",
      "Epoch 6: 100%|██████████| 48814/48814 [04:18<00:00, 188.77it/s, train_loss=0.00361]\n",
      "Epoch 6: \n",
      "Losses: train = 0.00345724, test = 0.01820503\n",
      "Test: accuracy = 0.98957700\n",
      "Test: precision = 0.95378275\n",
      "Test: recall = 0.93990645\n",
      "Test: f1_score = 0.94642680\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 7: 100%|██████████| 48814/48814 [04:17<00:00, 189.58it/s, train_loss=0.0034] \n",
      "Epoch 7: \n",
      "Losses: train = 0.00339446, test = 0.01852499\n",
      "Test: accuracy = 0.98956012\n",
      "Test: precision = 0.95337612\n",
      "Test: recall = 0.94110056\n",
      "Test: f1_score = 0.94680078\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 8: 100%|██████████| 48814/48814 [04:19<00:00, 187.90it/s, train_loss=0.00337]\n",
      "Epoch 8: \n",
      "Losses: train = 0.00326879, test = 0.01847182\n",
      "Test: accuracy = 0.98955168\n",
      "Test: precision = 0.95394198\n",
      "Test: recall = 0.94028552\n",
      "Test: f1_score = 0.94668026\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 9: 100%|██████████| 48814/48814 [04:18<00:00, 188.79it/s, train_loss=0.00342]\n",
      "Epoch 9: \n",
      "Losses: train = 0.00325291, test = 0.01850499\n",
      "Test: accuracy = 0.98956856\n",
      "Test: precision = 0.95181511\n",
      "Test: recall = 0.93975306\n",
      "Test: f1_score = 0.94550020\n",
      "BAD EPOCHS: 3 (<< >)\n",
      "Epoch 10: 100%|██████████| 48814/48814 [04:18<00:00, 189.00it/s, train_loss=0.00326]\n",
      "Epoch 10: \n",
      "Losses: train = 0.00316371, test = 0.01833921\n",
      "Test: accuracy = 0.98949261\n",
      "Test: precision = 0.95400066\n",
      "Test: recall = 0.94029491\n",
      "Test: f1_score = 0.94671400\n",
      "BAD EPOCHS: 4 (<< <)\n",
      "Epoch 11: 100%|██████████| 48814/48814 [04:18<00:00, 188.51it/s, train_loss=0.00318]\n",
      "Epoch 11: \n",
      "Losses: train = 0.00312096, test = 0.01900064\n",
      "Test: accuracy = 0.98958544\n",
      "Test: precision = 0.95415073\n",
      "Test: recall = 0.93968377\n",
      "Test: f1_score = 0.94646068\n",
      "BAD EPOCHS: 4 (<< >)\n",
      "Epoch 12: 100%|██████████| 48814/48814 [04:17<00:00, 189.62it/s, train_loss=0.00316]\n",
      "Epoch 12: \n",
      "Losses: train = 0.00308232, test = 0.01854344\n",
      "Test: accuracy = 0.98964452\n",
      "Test: precision = 0.95357000\n",
      "Test: recall = 0.94044710\n",
      "Test: f1_score = 0.94655334\n",
      "BAD EPOCHS: 4 (<< >)\n",
      "Epoch 13: 100%|██████████| 48814/48814 [04:18<00:00, 188.79it/s, train_loss=0.00301]\n",
      "Epoch 13: \n",
      "Losses: train = 0.00300982, test = 0.01895933\n",
      "Test: accuracy = 0.98947573\n",
      "Test: precision = 0.95353691\n",
      "Test: recall = 0.93764217\n",
      "Test: f1_score = 0.94493937\n",
      "BAD EPOCHS: 5 (<< <)\n",
      "Maximum bad epochs exceeded. Process has been stopped. Best score 0.9897964350820336 (on epoch 5)\n",
      "\n",
      "MODEL TUNING\n",
      "Loading state_dict... done.\n",
      "Epoch 1: 100%|██████████| 48814/48814 [04:16<00:00, 190.24it/s, train_loss=0.00337]\n",
      "Epoch 1: \n",
      "Losses: train = 0.00338343, test = 0.01858033\n",
      "Test: accuracy = 0.98972048\n",
      "Test: precision = 0.95458358\n",
      "Test: recall = 0.93861243\n",
      "Test: f1_score = 0.94602836\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 2: 100%|██████████| 48814/48814 [04:16<00:00, 189.98it/s, train_loss=0.00325]\n",
      "Epoch 2: \n",
      "Losses: train = 0.00331677, test = 0.01821243\n",
      "Test: accuracy = 0.98972892\n",
      "Test: precision = 0.95476130\n",
      "Test: recall = 0.94018105\n",
      "Test: f1_score = 0.94699169\n",
      "BAD EPOCHS: 1 (<< >)\n",
      "Epoch 3: 100%|██████████| 48814/48814 [04:16<00:00, 189.96it/s, train_loss=0.00317]\n",
      "Epoch 3: \n",
      "Losses: train = 0.00332627, test = 0.01868815\n",
      "Test: accuracy = 0.98968672\n",
      "Test: precision = 0.95462722\n",
      "Test: recall = 0.93847935\n",
      "Test: f1_score = 0.94598069\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 4: 100%|██████████| 48814/48814 [04:17<00:00, 189.88it/s, train_loss=0.00316]\n",
      "Epoch 4: \n",
      "Losses: train = 0.00325084, test = 0.01793492\n",
      "Test: accuracy = 0.98968672\n",
      "Test: precision = 0.95471189\n",
      "Test: recall = 0.94014507\n",
      "Test: f1_score = 0.94694793\n",
      "BAD EPOCHS: 3 (<< =)\n",
      "Epoch 5: 100%|██████████| 48814/48814 [04:15<00:00, 190.78it/s, train_loss=0.00339]\n",
      "Epoch 5: \n",
      "Losses: train = 0.00329868, test = 0.01814992\n",
      "Test: accuracy = 0.98967828\n",
      "Test: precision = 0.95470848\n",
      "Test: recall = 0.94009050\n",
      "Test: f1_score = 0.94691857\n",
      "BAD EPOCHS: 4 (<< <)\n",
      "Epoch 6: 100%|██████████| 48814/48814 [04:15<00:00, 191.32it/s, train_loss=0.00325]\n",
      "Epoch 6: \n",
      "Losses: train = 0.00330329, test = 0.01832204\n",
      "Test: accuracy = 0.98963608\n",
      "Test: precision = 0.95462143\n",
      "Test: recall = 0.94002182\n",
      "Test: f1_score = 0.94684244\n",
      "BAD EPOCHS: 5 (<< <)\n",
      "Maximum bad epochs exceeded. Process has been stopped. No models could surpass `best_score=0.9897964350820336` given\n",
      "\n",
      "=== UPOS TAGGER TRAINING HAS FINISHED ===\n",
      "\n",
      "Use the `.load('upos_model')` method to start working with the UPOS tagger.\n"
     ]
    }
   ],
   "source": [
    "tagger = UposTagger()\n",
    "tagger.load_train_corpus(corpus.train)\n",
    "tagger.load_test_corpus(corpus.dev)\n",
    "\n",
    "_ = tagger.train(MODEL_FN, device='cuda:8', word_emb_type='bert',\n",
    "                 word_emb_path=MODEL_FN.replace('model', BERT_MODEL_FN)\n",
    "                             + '_len512_ep3_bat8_seed42',\n",
    "                 word_emb_tune_params={\n",
    "                     'model_name': BERT_MODEL_FN,\n",
    "                     'max_len': 512, 'epochs': 3, 'batch_size': 8\n",
    "                 },\n",
    "                 rnn_emb_dim=None, cnn_emb_dim=None, cnn_kernels=range(1, 7),\n",
    "                 lstm_layers=3, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... done.\n",
      "Creating model... done.\n",
      "Loading state_dict... done.\n",
      "Fit corpus dict... done.\n"
     ]
    }
   ],
   "source": [
    "res_dev = 'corpora/_dev_' + MODEL_FN.replace('_model', '.conllu')\n",
    "res_test = 'corpora/_test_' + MODEL_FN.replace('_model', '.conllu')\n",
    "tagger = UposTagger()\n",
    "tagger.load(MODEL_FN)\n",
    "junky.clear_tqdm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6584                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing\n",
      "100%|██████████| 103/103 [00:10<00:00,  9.68it/s]\n",
      "Adjusting\n",
      "100%|██████████| 6584/6584 [00:23<00:00, 275.34it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.predict(corpus.dev, clone_ds=True, save_to=res_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating UPOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=====> 5500                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus\n",
      "  0%|          | 0/6584 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6584/6584 [01:08<00:00, 95.47it/s] \n",
      "UPOS total: 118488\n",
      "   correct: 117279\n",
      "     wrong: 1209\n",
      "  Accuracy: 0.9897964350820336\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating UPOS\n",
      "Load corpus\n",
      "[> 700                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[> 700                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==> 2400                                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[==> 2400                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===> 3700                                                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=====> 5200                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====> 5900                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=====> 5900                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n",
      "UPOS total: 118488\n",
      "   correct: 117279\n",
      "     wrong: 1209\n",
      "  Accuracy: 0.9897964350820336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.dev, res_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus\n",
      "100%|██████████| 6491/6491 [01:07<00:00, 95.92it/s] \n"
     ]
    }
   ],
   "source": [
    "_ = tagger.predict(corpus.test, save_to=res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating UPOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing\n",
      "100%|██████████| 102/102 [00:10<00:00,  9.74it/s]\n",
      "Adjusting\n",
      "100%|██████████| 6491/6491 [00:23<00:00, 274.60it/s]\n",
      "UPOS total: 117329\n",
      "   correct: 116337\n",
      "     wrong: 992\n",
      "  Accuracy: 0.9915451422921869\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.test, clone_ds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating UPOS\n",
      "Load corpus\n",
      "[=> 1200                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=> 1200                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==> 2300                                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[==> 2300                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===> 3500                                                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[====> 5000                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n",
      "UPOS total: 117329\n",
      "   correct: 116337\n",
      "     wrong: 992\n",
      "  Accuracy: 0.9915451422921869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.test, res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n",
      "ADJ: 0.9703829634931997\n",
      "ADP: 0.9993054952686865\n",
      "ADV: 0.9627454270850814\n",
      "AUX: 0.9485500467726847\n",
      "CCONJ: 0.9847182425978988\n",
      "DET: 0.9493670886075949\n",
      "INTJ: 0.5\n",
      "NOUN: 0.9901998262380539\n",
      "NUM: 0.9709558823529412\n",
      "PART: 0.9644970414201184\n",
      "PRON: 0.97059400117624\n",
      "PROPN: 0.9657308009909166\n",
      "PUNCT: 0.9996726830636865\n",
      "SCONJ: 0.9476190476190476\n",
      "SYM: 1.0\n",
      "VERB: 0.982405140758874\n",
      "X: 0.6875\n"
     ]
    }
   ],
   "source": [
    "corp_gold = list(corpus.test())\n",
    "corp_test = list(tagger._get_corpus(res_test))\n",
    "tags = sorted(set(x['UPOS'] for x in corp_gold\n",
    "                            for x in x[0] if x['UPOS']))\n",
    "for tag in tags:\n",
    "    print('{}: {}'.format(\n",
    "        tag, tagger.evaluate(corp_gold, corp_test,\n",
    "                             label=tag, log_file=None)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading UniversalDependencies/UD_Russian-SynTagRus contents\n",
      "done: 9525 bytes\n"
     ]
    }
   ],
   "source": [
    "from corpuscula.corpus_utils import download_ud, UniversalDependencies, \\\n",
    "                                    AdjustedForSpeech\n",
    "import junky\n",
    "from mordl import FeatsTagger\n",
    "\n",
    "BERT_MODEL_FN = 'bert-base-multilingual-cased'\n",
    "MODEL_FN = 'feats_model'\n",
    "SEED=42\n",
    "\n",
    "# we use UD Taiga corpus only as example. For real model training comment\n",
    "# Taiga and uncomment SynTagRus\n",
    "#corpus_name = 'UD_Russian-Taiga'\n",
    "corpus_name = 'UD_Russian-SynTagRus'\n",
    "download_ud(corpus_name, overwrite=False)\n",
    "corpus = UniversalDependencies(corpus_name)\n",
    "#corpus = AdjustedForSpeech(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Load corpus\n",
      "[=================================================] 48814           \n",
      "Corpus has been loaded: 48814 sentences, 871526 tokens\n",
      "Test: Load corpus\n",
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATS TAGGER TRAINING PIPELINE ===\n",
      "\n",
      "BERT MODEL TUNING 'bert-base-multilingual-cased'. The result's model name will be 'feats_bert-base-multilingual-cased_len512_ep3_bat8_seed42'\n",
      "Loading tokenizer...\n",
      "Tokenizer is loaded. Vocab size: 119547\n",
      "Corpora processing... done.\n",
      "Loading model 'bert-base-multilingual-cased'... done.\n",
      "Epoch 1: 100%|██████████| 48814/48814 [12:28<00:00, 65.23it/s, train_loss=0.203] \n",
      "Average train loss: 0.48633251200094013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: accuracy = 0.94740854\n",
      "Dev: precision = 0.61015736\n",
      "Dev: recall = 0.63154997\n",
      "Dev: f1_score = 0.60670891\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to feats_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "Epoch 2: 100%|██████████| 48814/48814 [12:28<00:00, 65.25it/s, train_loss=0.177]  \n",
      "Average train loss: 0.13621492406871671\n",
      "Dev: accuracy = 0.96035391\n",
      "Dev: precision = 0.72256610\n",
      "Dev: recall = 0.74398063\n",
      "Dev: f1_score = 0.72458306\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to feats_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "Epoch 3: 100%|██████████| 48814/48814 [12:28<00:00, 65.18it/s, train_loss=0.0404] \n",
      "Average train loss: 0.07456367332809576\n",
      "Dev: accuracy = 0.96498032\n",
      "Dev: precision = 0.76500494\n",
      "Dev: recall = 0.77045950\n",
      "Dev: f1_score = 0.75925111\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to feats_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "\n",
      "DATASETS CREATION\n",
      "Tokenizing\n",
      "100%|██████████| 48814/48814 [00:32<00:00, 1483.01it/s]\n",
      "Vectorizing\n",
      "100%|██████████| 763/763 [01:16<00:00,  9.95it/s]\n",
      "Adjusting\n",
      "100%|██████████| 48814/48814 [02:53<00:00, 282.07it/s]\n",
      "Vectorizing\n",
      "100%|██████████| 103/103 [00:10<00:00,  9.63it/s]\n",
      "Adjusting\n",
      "100%|██████████| 6584/6584 [00:23<00:00, 283.49it/s]\n",
      "\n",
      "MODEL CREATION\n",
      "Config saved\n",
      "\n",
      "MODEL TRAINING\n",
      "Epoch 1: 100%|██████████| 48814/48814 [04:23<00:00, 185.00it/s, train_loss=0.0332]\n",
      "Epoch 1: \n",
      "Losses: train = 0.13771697, test = 0.04070414\n",
      "Test: accuracy = 0.97849571\n",
      "Test: precision = 0.64283393\n",
      "Test: recall = 0.64986066\n",
      "Test: f1_score = 0.63804004\n",
      "new maximum score 0.97849571\n",
      "Saving state_dict... done.\n",
      "Epoch 2: 100%|██████████| 48814/48814 [04:23<00:00, 185.24it/s, train_loss=0.0192]\n",
      "Epoch 2: \n",
      "Losses: train = 0.02216711, test = 0.03678166\n",
      "Test: accuracy = 0.98055499\n",
      "Test: precision = 0.75694685\n",
      "Test: recall = 0.76143620\n",
      "Test: f1_score = 0.75185201\n",
      "new maximum score 0.98055499\n",
      "Saving state_dict... done.\n",
      "Epoch 3: 100%|██████████| 48814/48814 [04:24<00:00, 184.75it/s, train_loss=0.0157]\n",
      "Epoch 3: \n",
      "Losses: train = 0.01640385, test = 0.03418844\n",
      "Test: accuracy = 0.98150868\n",
      "Test: precision = 0.79346413\n",
      "Test: recall = 0.80198704\n",
      "Test: f1_score = 0.79073985\n",
      "new maximum score 0.98150868\n",
      "Saving state_dict... done.\n",
      "Epoch 4: 100%|██████████| 48814/48814 [04:23<00:00, 184.93it/s, train_loss=0.0142]\n",
      "Epoch 4: \n",
      "Losses: train = 0.01404710, test = 0.03307209\n",
      "Test: accuracy = 0.98200662\n",
      "Test: precision = 0.82329432\n",
      "Test: recall = 0.82066936\n",
      "Test: f1_score = 0.81536187\n",
      "new maximum score 0.98200662\n",
      "Saving state_dict... done.\n",
      "Epoch 5: 100%|██████████| 48814/48814 [04:24<00:00, 184.48it/s, train_loss=0.013] \n",
      "Epoch 5: \n",
      "Losses: train = 0.01284785, test = 0.03278089\n",
      "Test: accuracy = 0.98223449\n",
      "Test: precision = 0.84493125\n",
      "Test: recall = 0.84213522\n",
      "Test: f1_score = 0.83769569\n",
      "new maximum score 0.98223449\n",
      "Saving state_dict... done.\n",
      "Epoch 6: 100%|██████████| 48814/48814 [04:23<00:00, 185.50it/s, train_loss=0.0118]\n",
      "Epoch 6: \n",
      "Losses: train = 0.01219420, test = 0.03306702\n",
      "Test: accuracy = 0.98232732\n",
      "Test: precision = 0.84243757\n",
      "Test: recall = 0.84179025\n",
      "Test: f1_score = 0.83562684\n",
      "new maximum score 0.98232732\n",
      "Saving state_dict... done.\n",
      "Epoch 7: 100%|██████████| 48814/48814 [04:24<00:00, 184.87it/s, train_loss=0.0115]\n",
      "Epoch 7: \n",
      "Losses: train = 0.01161192, test = 0.03306057\n",
      "Test: accuracy = 0.98229357\n",
      "Test: precision = 0.84471321\n",
      "Test: recall = 0.84034703\n",
      "Test: f1_score = 0.83601685\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 8: 100%|██████████| 48814/48814 [04:23<00:00, 185.21it/s, train_loss=0.0111]\n",
      "Epoch 8: \n",
      "Losses: train = 0.01112603, test = 0.03206825\n",
      "Test: accuracy = 0.98239484\n",
      "Test: precision = 0.84641677\n",
      "Test: recall = 0.84558657\n",
      "Test: f1_score = 0.84019725\n",
      "new maximum score 0.98239484\n",
      "Saving state_dict... done.\n",
      "Epoch 9: 100%|██████████| 48814/48814 [04:23<00:00, 185.01it/s, train_loss=0.0106]\n",
      "Epoch 9: \n",
      "Losses: train = 0.01072185, test = 0.03180548\n",
      "Test: accuracy = 0.98232732\n",
      "Test: precision = 0.84730283\n",
      "Test: recall = 0.84552591\n",
      "Test: f1_score = 0.83995222\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 10: 100%|██████████| 48814/48814 [04:39<00:00, 174.70it/s, train_loss=0.0114] \n",
      "Epoch 10: \n",
      "Losses: train = 0.01050562, test = 0.03141775\n",
      "Test: accuracy = 0.98242860\n",
      "Test: precision = 0.84454734\n",
      "Test: recall = 0.84131225\n",
      "Test: f1_score = 0.83704303\n",
      "new maximum score 0.98242860\n",
      "Saving state_dict... done.\n",
      "Epoch 11: 100%|██████████| 48814/48814 [04:23<00:00, 184.90it/s, train_loss=0.0103]\n",
      "Epoch 11: \n",
      "Losses: train = 0.01033383, test = 0.03132869\n",
      "Test: accuracy = 0.98259739\n",
      "Test: precision = 0.85161460\n",
      "Test: recall = 0.84733400\n",
      "Test: f1_score = 0.84392237\n",
      "new maximum score 0.98259739\n",
      "Saving state_dict... done.\n",
      "Epoch 12: 100%|██████████| 48814/48814 [04:22<00:00, 185.60it/s, train_loss=0.0104] \n",
      "Epoch 12: \n",
      "Losses: train = 0.01004319, test = 0.03241584\n",
      "Test: accuracy = 0.98251300\n",
      "Test: precision = 0.85109968\n",
      "Test: recall = 0.84803307\n",
      "Test: f1_score = 0.84411578\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 13: 100%|██████████| 48814/48814 [04:23<00:00, 185.52it/s, train_loss=0.00971]\n",
      "Epoch 13: \n",
      "Losses: train = 0.00980536, test = 0.03160101\n",
      "Test: accuracy = 0.98268179\n",
      "Test: precision = 0.85261267\n",
      "Test: recall = 0.85187536\n",
      "Test: f1_score = 0.84706543\n",
      "new maximum score 0.98268179\n",
      "Saving state_dict... done.\n",
      "Epoch 14: 100%|██████████| 48814/48814 [04:24<00:00, 184.59it/s, train_loss=0.0097] \n",
      "Epoch 14: \n",
      "Losses: train = 0.00968156, test = 0.03189258\n",
      "Test: accuracy = 0.98269023\n",
      "Test: precision = 0.85316894\n",
      "Test: recall = 0.84824004\n",
      "Test: f1_score = 0.84517023\n",
      "new maximum score 0.98269023\n",
      "Saving state_dict... done.\n",
      "Epoch 15: 100%|██████████| 48814/48814 [04:30<00:00, 67.14it/s, train_loss=0.00944] \n",
      "Epoch 15: \n",
      "Losses: train = 0.00953842, test = 0.03220822\n",
      "Test: accuracy = 0.98246236\n",
      "Test: precision = 0.85512718\n",
      "Test: recall = 0.84944010\n",
      "Test: f1_score = 0.84622674\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 16: 100%|██████████| 48814/48814 [04:23<00:00, 185.20it/s, train_loss=0.00977]\n",
      "Epoch 16: \n",
      "Losses: train = 0.00952967, test = 0.03135403\n",
      "Test: accuracy = 0.98269867\n",
      "Test: precision = 0.85097751\n",
      "Test: recall = 0.84497352\n",
      "Test: f1_score = 0.84262532\n",
      "new maximum score 0.98269867\n",
      "Saving state_dict... done.\n",
      "Epoch 17: 100%|██████████| 48814/48814 [04:24<00:00, 184.63it/s, train_loss=0.00929]\n",
      "Epoch 17: \n",
      "Losses: train = 0.00939725, test = 0.03181904\n",
      "Test: accuracy = 0.98248768\n",
      "Test: precision = 0.85486956\n",
      "Test: recall = 0.85282034\n",
      "Test: f1_score = 0.84881321\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 18: 100%|██████████| 48814/48814 [04:23<00:00, 185.21it/s, train_loss=0.0092] \n",
      "Epoch 18: \n",
      "Losses: train = 0.00923024, test = 0.03188368\n",
      "Test: accuracy = 0.98231888\n",
      "Test: precision = 0.84997112\n",
      "Test: recall = 0.84655845\n",
      "Test: f1_score = 0.84259502\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 19: 100%|██████████| 48814/48814 [04:23<00:00, 184.98it/s, train_loss=0.00921]\n",
      "Epoch 19: \n",
      "Losses: train = 0.00903493, test = 0.03167814\n",
      "Test: accuracy = 0.98256364\n",
      "Test: precision = 0.85630384\n",
      "Test: recall = 0.85409887\n",
      "Test: f1_score = 0.84987611\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 20: 100%|██████████| 48814/48814 [04:24<00:00, 184.39it/s, train_loss=0.00897]\n",
      "Epoch 20: \n",
      "Losses: train = 0.00897523, test = 0.03143909\n",
      "Test: accuracy = 0.98258051\n",
      "Test: precision = 0.85858686\n",
      "Test: recall = 0.85322234\n",
      "Test: f1_score = 0.85019748\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 21: 100%|██████████| 48814/48814 [04:25<00:00, 184.08it/s, train_loss=0.00892]\n",
      "Epoch 21: \n",
      "Losses: train = 0.00881357, test = 0.03186844\n",
      "Test: accuracy = 0.98263115\n",
      "Test: precision = 0.85552413\n",
      "Test: recall = 0.84835934\n",
      "Test: f1_score = 0.84629367\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 22: 100%|██████████| 48814/48814 [04:24<00:00, 184.60it/s, train_loss=0.00834]\n",
      "Epoch 22: \n",
      "Losses: train = 0.00884172, test = 0.03106765\n",
      "Test: accuracy = 0.98258895\n",
      "Test: precision = 0.86220460\n",
      "Test: recall = 0.85187568\n",
      "Test: f1_score = 0.85124860\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 23: 100%|██████████| 48814/48814 [04:22<00:00, 186.01it/s, train_loss=0.00845]\n",
      "Epoch 23: \n",
      "Losses: train = 0.00882381, test = 0.03194800\n",
      "Test: accuracy = 0.98253832\n",
      "Test: precision = 0.86128070\n",
      "Test: recall = 0.85775219\n",
      "Test: f1_score = 0.85454830\n",
      "BAD EPOCHS: 4 (<< <)\n",
      "Epoch 24: 100%|██████████| 48814/48814 [04:22<00:00, 186.13it/s, train_loss=0.0088] \n",
      "Epoch 24: \n",
      "Losses: train = 0.00868926, test = 0.03143020\n",
      "Test: accuracy = 0.98274087\n",
      "Test: precision = 0.85760377\n",
      "Test: recall = 0.85149739\n",
      "Test: f1_score = 0.85000271\n",
      "new maximum score 0.98274087\n",
      "Saving state_dict... done.\n",
      "Epoch 25: 100%|██████████| 48814/48814 [04:24<00:00, 184.81it/s, train_loss=0.00886]\n",
      "Epoch 25: \n",
      "Losses: train = 0.00863232, test = 0.03133311\n",
      "Test: accuracy = 0.98254676\n",
      "Test: precision = 0.85944630\n",
      "Test: recall = 0.85480497\n",
      "Test: f1_score = 0.85164221\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 26: 100%|██████████| 48814/48814 [04:24<00:00, 184.72it/s, train_loss=0.00852]\n",
      "Epoch 26: \n",
      "Losses: train = 0.00852525, test = 0.03126193\n",
      "Test: accuracy = 0.98283370\n",
      "Test: precision = 0.86311774\n",
      "Test: recall = 0.85791101\n",
      "Test: f1_score = 0.85546654\n",
      "new maximum score 0.98283370\n",
      "Saving state_dict... done.\n",
      "Epoch 27: 100%|██████████| 48814/48814 [04:23<00:00, 185.42it/s, train_loss=0.00877]\n",
      "Epoch 27: \n",
      "Losses: train = 0.00836317, test = 0.03258905\n",
      "Test: accuracy = 0.98261427\n",
      "Test: precision = 0.85772303\n",
      "Test: recall = 0.85083724\n",
      "Test: f1_score = 0.84907687\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 28: 100%|██████████| 48814/48814 [04:23<00:00, 185.60it/s, train_loss=0.00847]\n",
      "Epoch 28: \n",
      "Losses: train = 0.00828799, test = 0.03209171\n",
      "Test: accuracy = 0.98250456\n",
      "Test: precision = 0.86231321\n",
      "Test: recall = 0.86115259\n",
      "Test: f1_score = 0.85739659\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 29: 100%|██████████| 48814/48814 [04:23<00:00, 185.48it/s, train_loss=0.00829]\n",
      "Epoch 29: \n",
      "Losses: train = 0.00834981, test = 0.03216113\n",
      "Test: accuracy = 0.98269867\n",
      "Test: precision = 0.85938974\n",
      "Test: recall = 0.85291810\n",
      "Test: f1_score = 0.85035015\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 30: 100%|██████████| 48814/48814 [04:23<00:00, 185.24it/s, train_loss=0.00812]\n",
      "Epoch 30: \n",
      "Losses: train = 0.00820149, test = 0.03207266\n",
      "Test: accuracy = 0.98249612\n",
      "Test: precision = 0.86032235\n",
      "Test: recall = 0.85539641\n",
      "Test: f1_score = 0.85229503\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 31: 100%|██████████| 48814/48814 [04:23<00:00, 185.04it/s, train_loss=0.00826]\n",
      "Epoch 31: \n",
      "Losses: train = 0.00799745, test = 0.03146467\n",
      "Test: accuracy = 0.98247080\n",
      "Test: precision = 0.86954018\n",
      "Test: recall = 0.86517132\n",
      "Test: f1_score = 0.86217422\n",
      "BAD EPOCHS: 4 (<< <)\n",
      "Epoch 32: 100%|██████████| 48814/48814 [04:24<00:00, 184.79it/s, train_loss=0.00785]\n",
      "Epoch 32: \n",
      "Losses: train = 0.00808413, test = 0.03164146\n",
      "Test: accuracy = 0.98241172\n",
      "Test: precision = 0.86264669\n",
      "Test: recall = 0.85662129\n",
      "Test: f1_score = 0.85368224\n",
      "BAD EPOCHS: 5 (<< <)\n",
      "Maximum bad epochs exceeded. Process has been stopped. Best score 0.9828337046789548 (on epoch 26)\n",
      "\n",
      "MODEL TUNING\n",
      "Loading state_dict... done.\n",
      "Epoch 1: 100%|██████████| 48814/48814 [04:20<00:00, 187.05it/s, train_loss=0.00777]\n",
      "Epoch 1: \n",
      "Losses: train = 0.00814865, test = 0.03149628\n",
      "Test: accuracy = 0.98288434\n",
      "Test: precision = 0.86312405\n",
      "Test: recall = 0.85770410\n",
      "Test: f1_score = 0.85537208\n",
      "new maximum score 0.98288434\n",
      "Saving state_dict... done.\n",
      "Epoch 2: 100%|██████████| 48814/48814 [04:21<00:00, 187.00it/s, train_loss=0.00839]\n",
      "Epoch 2: \n",
      "Losses: train = 0.00801550, test = 0.03146790\n",
      "Test: accuracy = 0.98295186\n",
      "Test: precision = 0.86691735\n",
      "Test: recall = 0.86142312\n",
      "Test: f1_score = 0.85910495\n",
      "new maximum score 0.98295186\n",
      "Saving state_dict... done.\n",
      "Epoch 3: 100%|██████████| 48814/48814 [04:20<00:00, 187.55it/s, train_loss=0.00814]\n",
      "Epoch 3: \n",
      "Losses: train = 0.00814045, test = 0.03124775\n",
      "Test: accuracy = 0.98294342\n",
      "Test: precision = 0.86695278\n",
      "Test: recall = 0.86139621\n",
      "Test: f1_score = 0.85912074\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 4: 100%|██████████| 48814/48814 [04:21<00:00, 186.74it/s, train_loss=0.00758]\n",
      "Epoch 4: \n",
      "Losses: train = 0.00790434, test = 0.03156110\n",
      "Test: accuracy = 0.98294342\n",
      "Test: precision = 0.86865423\n",
      "Test: recall = 0.86322023\n",
      "Test: f1_score = 0.86088650\n",
      "BAD EPOCHS: 2 (<< =)\n",
      "Epoch 5: 100%|██████████| 48814/48814 [04:21<00:00, 186.94it/s, train_loss=0.00817]\n",
      "Epoch 5: \n",
      "Losses: train = 0.00802282, test = 0.03191475\n",
      "Test: accuracy = 0.98292654\n",
      "Test: precision = 0.86500245\n",
      "Test: recall = 0.85952081\n",
      "Test: f1_score = 0.85721824\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 6: 100%|██████████| 48814/48814 [04:20<00:00, 187.65it/s, train_loss=0.00774]\n",
      "Epoch 6: \n",
      "Losses: train = 0.00786794, test = 0.03168285\n",
      "Test: accuracy = 0.98300250\n",
      "Test: precision = 0.86885219\n",
      "Test: recall = 0.86338490\n",
      "Test: f1_score = 0.86106950\n",
      "new maximum score 0.98300250\n",
      "Saving state_dict... done.\n",
      "Epoch 7: 100%|██████████| 48814/48814 [04:20<00:00, 187.54it/s, train_loss=0.00758]\n",
      "Epoch 7: \n",
      "Losses: train = 0.00787705, test = 0.03145953\n",
      "Test: accuracy = 0.98301938\n",
      "Test: precision = 0.86874763\n",
      "Test: recall = 0.86335437\n",
      "Test: f1_score = 0.86100027\n",
      "new maximum score 0.98301938\n",
      "Saving state_dict... done.\n",
      "Epoch 8: 100%|██████████| 48814/48814 [04:20<00:00, 187.69it/s, train_loss=0.00827]\n",
      "Epoch 8: \n",
      "Losses: train = 0.00786054, test = 0.03097388\n",
      "Test: accuracy = 0.98302782\n",
      "Test: precision = 0.86717148\n",
      "Test: recall = 0.86222593\n",
      "Test: f1_score = 0.85975864\n",
      "new maximum score 0.98302782\n",
      "Saving state_dict... done.\n",
      "Epoch 9: 100%|██████████| 48814/48814 [04:21<00:00, 187.02it/s, train_loss=0.00766]\n",
      "Epoch 9: \n",
      "Losses: train = 0.00784424, test = 0.03132393\n",
      "Test: accuracy = 0.98300250\n",
      "Test: precision = 0.86873549\n",
      "Test: recall = 0.86345251\n",
      "Test: f1_score = 0.86105895\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 10: 100%|██████████| 48814/48814 [04:21<00:00, 186.64it/s, train_loss=0.00823]\n",
      "Epoch 10: \n",
      "Losses: train = 0.00790399, test = 0.03081513\n",
      "Test: accuracy = 0.98296030\n",
      "Test: precision = 0.86673959\n",
      "Test: recall = 0.86152240\n",
      "Test: f1_score = 0.85911431\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 11: 100%|██████████| 48814/48814 [04:20<00:00, 187.20it/s, train_loss=0.00777]\n",
      "Epoch 11: \n",
      "Losses: train = 0.00781208, test = 0.03169805\n",
      "Test: accuracy = 0.98297718\n",
      "Test: precision = 0.86874360\n",
      "Test: recall = 0.86341638\n",
      "Test: f1_score = 0.86104287\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 12: 100%|██████████| 48814/48814 [04:21<00:00, 186.97it/s, train_loss=0.00791]\n",
      "Epoch 12: \n",
      "Losses: train = 0.00776249, test = 0.03179831\n",
      "Test: accuracy = 0.98301938\n",
      "Test: precision = 0.87070883\n",
      "Test: recall = 0.86590172\n",
      "Test: f1_score = 0.86333930\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 13: 100%|██████████| 48814/48814 [04:20<00:00, 187.48it/s, train_loss=0.00764]\n",
      "Epoch 13: \n",
      "Losses: train = 0.00786447, test = 0.03084303\n",
      "Test: accuracy = 0.98299406\n",
      "Test: precision = 0.86690256\n",
      "Test: recall = 0.86223235\n",
      "Test: f1_score = 0.85963318\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 14: 100%|██████████| 48814/48814 [04:19<00:00, 187.83it/s, train_loss=0.00831]\n",
      "Epoch 14: \n",
      "Losses: train = 0.00781497, test = 0.03153130\n",
      "Test: accuracy = 0.98300250\n",
      "Test: precision = 0.87063194\n",
      "Test: recall = 0.86577238\n",
      "Test: f1_score = 0.86322905\n",
      "BAD EPOCHS: 3 (<< >)\n",
      "Epoch 15: 100%|██████████| 48814/48814 [04:21<00:00, 186.97it/s, train_loss=0.00757]\n",
      "Epoch 15: \n",
      "Losses: train = 0.00781115, test = 0.03125978\n",
      "Test: accuracy = 0.98299406\n",
      "Test: precision = 0.86882989\n",
      "Test: recall = 0.86410982\n",
      "Test: f1_score = 0.86151786\n",
      "BAD EPOCHS: 4 (<< <)\n",
      "Epoch 16: 100%|██████████| 48814/48814 [04:19<00:00, 187.98it/s, train_loss=0.00771]\n",
      "Epoch 16: \n",
      "Losses: train = 0.00770422, test = 0.03073560\n",
      "Test: accuracy = 0.98301938\n",
      "Test: precision = 0.86714265\n",
      "Test: recall = 0.86229248\n",
      "Test: f1_score = 0.85977214\n",
      "BAD EPOCHS: 4 (<< >)\n",
      "Epoch 17: 100%|██████████| 48814/48814 [04:20<00:00, 187.51it/s, train_loss=0.00774]\n",
      "Epoch 17: \n",
      "Losses: train = 0.00778331, test = 0.03140634\n",
      "Test: accuracy = 0.98304470\n",
      "Test: precision = 0.86891368\n",
      "Test: recall = 0.86419728\n",
      "Test: f1_score = 0.86160554\n",
      "new maximum score 0.98304470\n",
      "Saving state_dict... done.\n",
      "Epoch 18: 100%|██████████| 48814/48814 [04:20<00:00, 187.55it/s, train_loss=0.00737]\n",
      "Epoch 18: \n",
      "Losses: train = 0.00768657, test = 0.03094698\n",
      "Test: accuracy = 0.98297718\n",
      "Test: precision = 0.86852613\n",
      "Test: recall = 0.86391113\n",
      "Test: f1_score = 0.86124627\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 19: 100%|██████████| 48814/48814 [04:21<00:00, 186.75it/s, train_loss=0.00784]\n",
      "Epoch 19: \n",
      "Losses: train = 0.00780318, test = 0.03087552\n",
      "Test: accuracy = 0.98299406\n",
      "Test: precision = 0.86848872\n",
      "Test: recall = 0.86385220\n",
      "Test: f1_score = 0.86120168\n",
      "BAD EPOCHS: 1 (<< >)\n",
      "Epoch 20: 100%|██████████| 48814/48814 [04:20<00:00, 187.34it/s, train_loss=0.00781]\n",
      "Epoch 20: \n",
      "Losses: train = 0.00774084, test = 0.03067023\n",
      "Test: accuracy = 0.98299406\n",
      "Test: precision = 0.86666094\n",
      "Test: recall = 0.86231839\n",
      "Test: f1_score = 0.85955849\n",
      "BAD EPOCHS: 2 (<< =)\n",
      "Epoch 21: 100%|██████████| 48814/48814 [04:19<00:00, 187.84it/s, train_loss=0.00778]\n",
      "Epoch 21: \n",
      "Losses: train = 0.00781742, test = 0.03060211\n",
      "Test: accuracy = 0.98303626\n",
      "Test: precision = 0.86671783\n",
      "Test: recall = 0.86237264\n",
      "Test: f1_score = 0.85961047\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 22: 100%|██████████| 48814/48814 [04:21<00:00, 186.65it/s, train_loss=0.00738]\n",
      "Epoch 22: \n",
      "Losses: train = 0.00765518, test = 0.03092047\n",
      "Test: accuracy = 0.98304470\n",
      "Test: precision = 0.86833864\n",
      "Test: recall = 0.86397735\n",
      "Test: f1_score = 0.86119356\n",
      "BAD EPOCHS: 2 (== >)\n",
      "Epoch 23: 100%|██████████| 48814/48814 [04:20<00:00, 187.12it/s, train_loss=0.00797]\n",
      "Epoch 23: \n",
      "Losses: train = 0.00772542, test = 0.03175077\n",
      "Test: accuracy = 0.98300250\n",
      "Test: precision = 0.87079050\n",
      "Test: recall = 0.86570601\n",
      "Test: f1_score = 0.86326995\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 24: 100%|██████████| 48814/48814 [04:21<00:00, 186.92it/s, train_loss=0.00781]\n",
      "Epoch 24: \n",
      "Losses: train = 0.00765536, test = 0.03134960\n",
      "Test: accuracy = 0.98305314\n",
      "Test: precision = 0.87013037\n",
      "Test: recall = 0.86576544\n",
      "Test: f1_score = 0.86297071\n",
      "new maximum score 0.98305314\n",
      "Saving state_dict... done.\n",
      "Epoch 25: 100%|██████████| 48814/48814 [04:20<00:00, 187.56it/s, train_loss=0.0074] \n",
      "Epoch 25: \n",
      "Losses: train = 0.00769385, test = 0.03113166\n",
      "Test: accuracy = 0.98307002\n",
      "Test: precision = 0.86848078\n",
      "Test: recall = 0.86408230\n",
      "Test: f1_score = 0.86131441\n",
      "new maximum score 0.98307002\n",
      "Saving state_dict... done.\n",
      "Epoch 26: 100%|██████████| 48814/48814 [04:20<00:00, 187.51it/s, train_loss=0.00776]\n",
      "Epoch 26: \n",
      "Losses: train = 0.00770830, test = 0.03060573\n",
      "Test: accuracy = 0.98307846\n",
      "Test: precision = 0.86618430\n",
      "Test: recall = 0.86256121\n",
      "Test: f1_score = 0.85939227\n",
      "new maximum score 0.98307846\n",
      "Saving state_dict... done.\n",
      "Epoch 27: 100%|██████████| 48814/48814 [04:20<00:00, 187.35it/s, train_loss=0.00801]\n",
      "Epoch 27: \n",
      "Losses: train = 0.00770967, test = 0.03098162\n",
      "Test: accuracy = 0.98301094\n",
      "Test: precision = 0.86795070\n",
      "Test: recall = 0.86414961\n",
      "Test: f1_score = 0.86103472\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 28: 100%|██████████| 48814/48814 [04:20<00:00, 187.23it/s, train_loss=0.00741]\n",
      "Epoch 28: \n",
      "Losses: train = 0.00750665, test = 0.03075731\n",
      "Test: accuracy = 0.98301094\n",
      "Test: precision = 0.86760794\n",
      "Test: recall = 0.86408282\n",
      "Test: f1_score = 0.86083374\n",
      "BAD EPOCHS: 2 (<< =)\n",
      "Epoch 29: 100%|██████████| 48814/48814 [04:19<00:00, 188.06it/s, train_loss=0.00754]\n",
      "Epoch 29: \n",
      "Losses: train = 0.00769152, test = 0.03066042\n",
      "Test: accuracy = 0.98301094\n",
      "Test: precision = 0.86775793\n",
      "Test: recall = 0.86400713\n",
      "Test: f1_score = 0.86086068\n",
      "BAD EPOCHS: 3 (<< =)\n",
      "Epoch 30: 100%|██████████| 48814/48814 [04:19<00:00, 187.97it/s, train_loss=0.00791]\n",
      "Epoch 30: \n",
      "Losses: train = 0.00763720, test = 0.03082814\n",
      "Test: accuracy = 0.98299406\n",
      "Test: precision = 0.86790351\n",
      "Test: recall = 0.86505840\n",
      "Test: f1_score = 0.86161963\n",
      "BAD EPOCHS: 4 (<< <)\n",
      "Epoch 31: 100%|██████████| 48814/48814 [04:21<00:00, 186.69it/s, train_loss=0.00788]\n",
      "Epoch 31: \n",
      "Losses: train = 0.00767354, test = 0.03077112\n",
      "Test: accuracy = 0.98301938\n",
      "Test: precision = 0.86728356\n",
      "Test: recall = 0.86422635\n",
      "Test: f1_score = 0.86077843\n",
      "BAD EPOCHS: 4 (<< >)\n",
      "Epoch 32: 100%|██████████| 48814/48814 [04:20<00:00, 187.38it/s, train_loss=0.0075] \n",
      "Epoch 32: \n",
      "Losses: train = 0.00764214, test = 0.03106662\n",
      "Test: accuracy = 0.98298562\n",
      "Test: precision = 0.86792116\n",
      "Test: recall = 0.86392165\n",
      "Test: f1_score = 0.86088807\n",
      "BAD EPOCHS: 5 (<< <)\n",
      "Maximum bad epochs exceeded. Process has been stopped. Best score 0.9830784552022146 (on epoch 26)\n",
      "\n",
      "=== FEATS TAGGER TRAINING HAS FINISHED ===\n",
      "\n",
      "Use the `.load('feats_model')` method to start working with the FEATS tagger.\n"
     ]
    }
   ],
   "source": [
    "tagger = FeatsTagger()\n",
    "tagger.load_train_corpus(corpus.train)\n",
    "tagger.load_test_corpus(corpus.dev)\n",
    "\n",
    "_ = tagger.train(MODEL_FN, device='cuda:8', word_emb_type='bert',\n",
    "                 word_emb_path=MODEL_FN.replace('model', BERT_MODEL_FN)\n",
    "                             + '_len512_ep3_bat8_seed42',\n",
    "                 word_emb_tune_params={\n",
    "                     'model_name': BERT_MODEL_FN,\n",
    "                     'max_len': 512, 'epochs': 3, 'batch_size': 8\n",
    "                 },\n",
    "                 rnn_emb_dim=None, cnn_emb_dim=None, cnn_kernels=range(1, 7),\n",
    "                 upos_emb_dim=200, lstm_layers=3, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... done.\n",
      "Creating model... done.\n",
      "Loading state_dict... done.\n",
      "Fit corpus dict... done.\n"
     ]
    }
   ],
   "source": [
    "res_dev = 'corpora/_dev_' + MODEL_FN.replace('_model', '.conllu')\n",
    "res_test = 'corpora/_test_' + MODEL_FN.replace('_model', '.conllu')\n",
    "tagger = FeatsTagger()\n",
    "tagger.load(MODEL_FN)\n",
    "junky.clear_tqdm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing\n",
      "100%|██████████| 103/103 [00:10<00:00,  9.65it/s]\n",
      "Adjusting\n",
      "100%|██████████| 6584/6584 [00:23<00:00, 280.21it/s]\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.predict(corpus.dev, clone_ds=True, save_to=res_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating FEATS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[==> 2700                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus\n",
      "  0%|          | 0/6584 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6584/6584 [01:09<00:00, 94.79it/s] \n",
      "FEATS total: 118488 tokens, 287191 tags\n",
      "    correct: 116483 tokens, 284084 tags\n",
      "      wrong: 2005 tokens, 3107 tags [471 excess / 467 absent / 2169 wrong type]\n",
      "   Accuracy: 0.972497325176264 / 0.9891814158521681\n",
      "[Total accuracy: 0.9830784552022146 / 0.9891814158521681]\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating FEATS\n",
      "Load corpus\n",
      "[> 600                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[> 600                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==> 3000                                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[==> 3000                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====> 4100                                                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[====> 4100                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====> 5200                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=====> 5200                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====> 5900                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=====> 5900                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n",
      "FEATS total: 118488 tokens, 287191 tags\n",
      "    correct: 116483 tokens, 284084 tags\n",
      "      wrong: 2005 tokens, 3107 tags [471 excess / 467 absent / 2169 wrong type]\n",
      "   Accuracy: 0.972497325176264 / 0.9891814158521681\n",
      "[Total accuracy: 0.9830784552022146 / 0.9891814158521681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.dev, res_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus\n",
      "100%|██████████| 6491/6491 [01:07<00:00, 95.57it/s] \n"
     ]
    }
   ],
   "source": [
    "_ = tagger.predict(corpus.test, save_to=res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating FEATS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing\n",
      "100%|██████████| 102/102 [00:10<00:00,  9.71it/s]\n",
      "Adjusting\n",
      "100%|██████████| 6491/6491 [00:23<00:00, 277.58it/s]\n",
      "FEATS total: 117329 tokens, 285121 tags\n",
      "    correct: 115310 tokens, 281884 tags\n",
      "      wrong: 2019 tokens, 3237 tags [465 excess / 553 absent / 2219 wrong type]\n",
      "   Accuracy: 0.9720627101523475 / 0.9886469253404695\n",
      "[Total accuracy: 0.982791978112828 / 0.9886469253404695]\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.test, clone_ds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating FEATS\n",
      "Load corpus\n",
      "[> 900                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[> 800                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==> 2200                                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[==> 2500                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===> 3400                                                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[===> 3700                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n",
      "FEATS total: 117329 tokens, 285121 tags\n",
      "    correct: 115310 tokens, 281884 tags\n",
      "      wrong: 2019 tokens, 3237 tags [465 excess / 553 absent / 2219 wrong type]\n",
      "   Accuracy: 0.9720627101523475 / 0.9886469253404695\n",
      "[Total accuracy: 0.982791978112828 / 0.9886469253404695]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.test, res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n",
      "Animacy: 0.9842530775514803\n",
      "Aspect: 0.9900425716141136\n",
      "Case: 0.9822159329942249\n",
      "Degree: 0.9970380901605355\n",
      "Foreign: 0.7090909090909091\n",
      "Gender: 0.9849553168812003\n",
      "Mood: 0.9962381057756141\n",
      "Number: 0.9931442663378545\n",
      "Person: 0.9970106761565837\n",
      "Polarity: 0.8695652173913043\n",
      "Tense: 0.9942892834835371\n",
      "Variant: 0.9962756052141527\n",
      "VerbForm: 0.9981239627678765\n",
      "Voice: 0.9887437766072589\n"
     ]
    }
   ],
   "source": [
    "corp_gold = list(corpus.test())\n",
    "corp_test = list(tagger._get_corpus(res_test))\n",
    "tags = sorted(set(x for x in corp_gold\n",
    "                    for x in x[0]\n",
    "                    for x in x['FEATS'].keys()))\n",
    "for tag in tags:\n",
    "    print('{}: {}'.format(\n",
    "        tag, tagger.evaluate(corp_gold, corp_test,\n",
    "                             feats=tag, log_file=None)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEMMA",
    "\n",
    "Note: For lemmata, consider to use *FastText* word embeddings",
    "instead of *BERT* (see the notebook `mordl-ft.ipynb`). Accuracy",
    "is much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading UniversalDependencies/UD_Russian-SynTagRus contents\n",
      "done: 9525 bytes\n"
     ]
    }
   ],
   "source": [
    "from corpuscula.corpus_utils import download_ud, UniversalDependencies, \\\n",
    "                                    AdjustedForSpeech\n",
    "import junky\n",
    "from mordl import LemmaTagger\n",
    "\n",
    "BERT_MODEL_FN = 'bert-base-multilingual-cased'\n",
    "MODEL_FN = 'lemma_model'\n",
    "SEED=42\n",
    "\n",
    "# we use UD Taiga corpus only as example. For real model training comment\n",
    "# Taiga and uncomment SynTagRus\n",
    "#corpus_name = 'UD_Russian-Taiga'\n",
    "corpus_name = 'UD_Russian-SynTagRus'\n",
    "download_ud(corpus_name, overwrite=False)\n",
    "corpus = UniversalDependencies(corpus_name)\n",
    "#corpus = AdjustedForSpeech(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Load corpus\n",
      "[=================================================] 48814           \n",
      "Corpus has been loaded: 48814 sentences, 871526 tokens\n",
      "Test: Load corpus\n",
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse corpus\n",
      "[#################################################] 100%           \n",
      "done: 48814 sentences, 862287 acceptable tokens (plus 321 for YO letters)\n",
      "Fit corpus dict... done.\n",
      "\n",
      "Preliminary trainset preparation:\n",
      "stage 1 of 3... done.\n",
      "stage 2 of 3... done.\n",
      "Lengths: [1883 {'allow_replace': True, 'allow_copy': True},\n",
      "          1881 {'allow_replace': True, 'allow_copy': False},\n",
      "          1915 {'allow_replace': False, 'allow_copy': True},\n",
      "          1881 {'allow_replace': False, 'allow_copy': False}]\n",
      "min = {'allow_replace': True, 'allow_copy': False}\n",
      "stage 3 of 3... done.\n",
      "\n",
      "=== LEMMA TAGGER TRAINING PIPELINE ===\n",
      "\n",
      "BERT MODEL TUNING 'bert-base-multilingual-cased'. The result's model name will be 'lemma_bert-base-multilingual-cased_len512_ep3_bat8_seed42'\n",
      "Loading tokenizer...\n",
      "Tokenizer is loaded. Vocab size: 119547\n",
      "Corpora processing... done.\n",
      "Loading model 'bert-base-multilingual-cased'... done.\n",
      "Epoch 1: 100%|██████████| 48814/48814 [12:27<00:00, 65.28it/s, train_loss=0.407] \n",
      "Average train loss: 0.5918087704975301\n",
      "Dev: accuracy = 0.94594105\n",
      "Dev: precision = 0.32544373\n",
      "Dev: recall = 0.31795232\n",
      "Dev: f1_score = 0.30515818\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to lemma_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "Epoch 2: 100%|██████████| 48814/48814 [12:26<00:00, 65.42it/s, train_loss=0.0126] \n",
      "Average train loss: 0.18349937712853046\n",
      "Dev: accuracy = 0.95972499\n",
      "Dev: precision = 0.45826901\n",
      "Dev: recall = 0.46922852\n",
      "Dev: f1_score = 0.44976366\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to lemma_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "Epoch 3: 100%|██████████| 48814/48814 [12:20<00:00, 65.89it/s, train_loss=0.251]  \n",
      "Average train loss: 0.11126927166153615\n",
      "Dev: accuracy = 0.96561878\n",
      "Dev: precision = 0.51464066\n",
      "Dev: recall = 0.52260044\n",
      "Dev: f1_score = 0.50508230\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to lemma_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "\n",
      "DATASETS CREATION\n",
      "Tokenizing\n",
      "100%|██████████| 48814/48814 [00:33<00:00, 1442.67it/s]\n",
      "Vectorizing\n",
      "100%|██████████| 763/763 [01:16<00:00,  9.95it/s]\n",
      "Adjusting\n",
      "100%|██████████| 48814/48814 [02:49<00:00, 288.26it/s]\n",
      "Vectorizing\n",
      "100%|██████████| 103/103 [00:10<00:00,  9.61it/s]\n",
      "Adjusting\n",
      "100%|██████████| 6584/6584 [00:22<00:00, 286.27it/s]\n",
      "\n",
      "MODEL CREATION\n",
      "Config saved\n",
      "\n",
      "MODEL TRAINING\n",
      "Epoch 1: 100%|██████████| 48814/48814 [04:32<00:00, 179.35it/s, train_loss=0.0476]\n",
      "Epoch 1: \n",
      "Losses: train = 0.14578597, test = 0.05051156\n",
      "Test: accuracy = 0.97994734\n",
      "Test: precision = 0.39413593\n",
      "Test: recall = 0.40646663\n",
      "Test: f1_score = 0.38827517\n",
      "new maximum score 0.97994734\n",
      "Saving state_dict... done.\n",
      "Epoch 2: 100%|██████████| 48814/48814 [04:34<00:00, 177.69it/s, train_loss=0.0283]\n",
      "Epoch 2: \n",
      "Losses: train = 0.03167603, test = 0.03927274\n",
      "Test: accuracy = 0.98436973\n",
      "Test: precision = 0.52166606\n",
      "Test: recall = 0.53615144\n",
      "Test: f1_score = 0.51739489\n",
      "new maximum score 0.98436973\n",
      "Saving state_dict... done.\n",
      "Epoch 3: 100%|██████████| 48814/48814 [04:31<00:00, 179.88it/s, train_loss=0.0223]\n",
      "Epoch 3: \n",
      "Losses: train = 0.02278577, test = 0.03461512\n",
      "Test: accuracy = 0.98618425\n",
      "Test: precision = 0.58673953\n",
      "Test: recall = 0.59617521\n",
      "Test: f1_score = 0.57999690\n",
      "new maximum score 0.98618425\n",
      "Saving state_dict... done.\n",
      "Epoch 4: 100%|██████████| 48814/48814 [04:32<00:00, 178.86it/s, train_loss=0.0185]\n",
      "Epoch 4: \n",
      "Losses: train = 0.01870137, test = 0.03306328\n",
      "Test: accuracy = 0.98674127\n",
      "Test: precision = 0.61042531\n",
      "Test: recall = 0.61613244\n",
      "Test: f1_score = 0.60183776\n",
      "new maximum score 0.98674127\n",
      "Saving state_dict... done.\n",
      "Epoch 5: 100%|██████████| 48814/48814 [04:32<00:00, 179.16it/s, train_loss=0.0159]\n",
      "Epoch 5: \n",
      "Losses: train = 0.01619335, test = 0.03054023\n",
      "Test: accuracy = 0.98772872\n",
      "Test: precision = 0.63510084\n",
      "Test: recall = 0.64245656\n",
      "Test: f1_score = 0.62911571\n",
      "new maximum score 0.98772872\n",
      "Saving state_dict... done.\n",
      "Epoch 6: 100%|██████████| 48814/48814 [04:33<00:00, 178.66it/s, train_loss=0.0142]\n",
      "Epoch 6: \n",
      "Losses: train = 0.01439302, test = 0.02959155\n",
      "Test: accuracy = 0.98802410\n",
      "Test: precision = 0.65245329\n",
      "Test: recall = 0.66032063\n",
      "Test: f1_score = 0.64621245\n",
      "new maximum score 0.98802410\n",
      "Saving state_dict... done.\n",
      "Epoch 7: 100%|██████████| 48814/48814 [04:32<00:00, 179.02it/s, train_loss=0.0131]\n",
      "Epoch 7: \n",
      "Losses: train = 0.01300627, test = 0.02877791\n",
      "Test: accuracy = 0.98849673\n",
      "Test: precision = 0.67180763\n",
      "Test: recall = 0.67195445\n",
      "Test: f1_score = 0.66245721\n",
      "new maximum score 0.98849673\n",
      "Saving state_dict... done.\n",
      "Epoch 8: 100%|██████████| 48814/48814 [04:35<00:00, 177.33it/s, train_loss=0.0121]\n",
      "Epoch 8: \n",
      "Losses: train = 0.01210043, test = 0.02862137\n",
      "Test: accuracy = 0.98859800\n",
      "Test: precision = 0.67549140\n",
      "Test: recall = 0.67733162\n",
      "Test: f1_score = 0.66689162\n",
      "new maximum score 0.98859800\n",
      "Saving state_dict... done.\n",
      "Epoch 9: 100%|██████████| 48814/48814 [04:35<00:00, 177.17it/s, train_loss=0.0111]\n",
      "Epoch 9: \n",
      "Losses: train = 0.01116160, test = 0.02679653\n",
      "Test: accuracy = 0.98913814\n",
      "Test: precision = 0.69132688\n",
      "Test: recall = 0.68985328\n",
      "Test: f1_score = 0.68269846\n",
      "new maximum score 0.98913814\n",
      "Saving state_dict... done.\n",
      "Epoch 10: 100%|██████████| 48814/48814 [04:34<00:00, 177.80it/s, train_loss=0.0104]\n",
      "Epoch 10: \n",
      "Losses: train = 0.01057678, test = 0.02686980\n",
      "Test: accuracy = 0.98903686\n",
      "Test: precision = 0.68791856\n",
      "Test: recall = 0.68807283\n",
      "Test: f1_score = 0.67859102\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 11: 100%|██████████| 48814/48814 [04:34<00:00, 177.64it/s, train_loss=0.0106] \n",
      "Epoch 11: \n",
      "Losses: train = 0.00999756, test = 0.02632730\n",
      "Test: accuracy = 0.98920566\n",
      "Test: precision = 0.69340229\n",
      "Test: recall = 0.69456691\n",
      "Test: f1_score = 0.68410149\n",
      "new maximum score 0.98920566\n",
      "Saving state_dict... done.\n",
      "Epoch 12: 100%|██████████| 48814/48814 [04:35<00:00, 177.17it/s, train_loss=0.00963]\n",
      "Epoch 12: \n",
      "Losses: train = 0.00948259, test = 0.02714980\n",
      "Test: accuracy = 0.98929849\n",
      "Test: precision = 0.70738455\n",
      "Test: recall = 0.70280763\n",
      "Test: f1_score = 0.69551156\n",
      "new maximum score 0.98929849\n",
      "Saving state_dict... done.\n",
      "Epoch 13: 100%|██████████| 48814/48814 [04:34<00:00, 177.64it/s, train_loss=0.00881]\n",
      "Epoch 13: \n",
      "Losses: train = 0.00902315, test = 0.02668395\n",
      "Test: accuracy = 0.98936601\n",
      "Test: precision = 0.69863451\n",
      "Test: recall = 0.69329106\n",
      "Test: f1_score = 0.68687333\n",
      "new maximum score 0.98936601\n",
      "Saving state_dict... done.\n",
      "Epoch 14: 100%|██████████| 48814/48814 [04:35<00:00, 177.41it/s, train_loss=0.00896]\n",
      "Epoch 14: \n",
      "Losses: train = 0.00853069, test = 0.02621275\n",
      "Test: accuracy = 0.98947573\n",
      "Test: precision = 0.71781431\n",
      "Test: recall = 0.71591511\n",
      "Test: f1_score = 0.70687145\n",
      "new maximum score 0.98947573\n",
      "Saving state_dict... done.\n",
      "Epoch 15: 100%|██████████| 48814/48814 [04:34<00:00, 177.64it/s, train_loss=0.00848]\n",
      "Epoch 15: \n",
      "Losses: train = 0.00823343, test = 0.02624989\n",
      "Test: accuracy = 0.98948417\n",
      "Test: precision = 0.71650869\n",
      "Test: recall = 0.70896698\n",
      "Test: f1_score = 0.70379538\n",
      "new maximum score 0.98948417\n",
      "Saving state_dict... done.\n",
      "Epoch 16: 100%|██████████| 48814/48814 [04:35<00:00, 177.29it/s, train_loss=0.00788]\n",
      "Epoch 16: \n",
      "Losses: train = 0.00777859, test = 0.02628712\n",
      "Test: accuracy = 0.98959388\n",
      "Test: precision = 0.72481795\n",
      "Test: recall = 0.71751059\n",
      "Test: f1_score = 0.71146166\n",
      "new maximum score 0.98959388\n",
      "Saving state_dict... done.\n",
      "Epoch 17: 100%|██████████| 48814/48814 [04:34<00:00, 177.78it/s, train_loss=0.00732]\n",
      "Epoch 17: \n",
      "Losses: train = 0.00747551, test = 0.02696016\n",
      "Test: accuracy = 0.98964452\n",
      "Test: precision = 0.72552145\n",
      "Test: recall = 0.71882567\n",
      "Test: f1_score = 0.71393344\n",
      "new maximum score 0.98964452\n",
      "Saving state_dict... done.\n",
      "Epoch 18: 100%|██████████| 48814/48814 [04:36<00:00, 176.86it/s, train_loss=0.00761]\n",
      "Epoch 18: \n",
      "Losses: train = 0.00721624, test = 0.02641353\n",
      "Test: accuracy = 0.98950105\n",
      "Test: precision = 0.72992155\n",
      "Test: recall = 0.71988978\n",
      "Test: f1_score = 0.71511193\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 19: 100%|██████████| 48814/48814 [04:35<00:00, 177.46it/s, train_loss=0.00707]\n",
      "Epoch 19: \n",
      "Losses: train = 0.00701762, test = 0.02645761\n",
      "Test: accuracy = 0.98988083\n",
      "Test: precision = 0.73205102\n",
      "Test: recall = 0.72218708\n",
      "Test: f1_score = 0.71798788\n",
      "new maximum score 0.98988083\n",
      "Saving state_dict... done.\n",
      "Epoch 20: 100%|██████████| 48814/48814 [04:34<00:00, 177.56it/s, train_loss=0.00659]\n",
      "Epoch 20: \n",
      "Losses: train = 0.00663856, test = 0.02648870\n",
      "Test: accuracy = 0.98991459\n",
      "Test: precision = 0.74533550\n",
      "Test: recall = 0.73659448\n",
      "Test: f1_score = 0.73197616\n",
      "new maximum score 0.98991459\n",
      "Saving state_dict... done.\n",
      "Epoch 21: 100%|██████████| 48814/48814 [04:35<00:00, 177.35it/s, train_loss=0.00651]\n",
      "Epoch 21: \n",
      "Losses: train = 0.00661538, test = 0.02695352\n",
      "Test: accuracy = 0.98988083\n",
      "Test: precision = 0.73503684\n",
      "Test: recall = 0.72404626\n",
      "Test: f1_score = 0.72081652\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 22: 100%|██████████| 48814/48814 [04:33<00:00, 178.52it/s, train_loss=0.00619]\n",
      "Epoch 22: \n",
      "Losses: train = 0.00622071, test = 0.02653814\n",
      "Test: accuracy = 0.98979644\n",
      "Test: precision = 0.72952147\n",
      "Test: recall = 0.72163206\n",
      "Test: f1_score = 0.71651144\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 23: 100%|██████████| 48814/48814 [04:32<00:00, 179.00it/s, train_loss=0.00643]\n",
      "Epoch 23: \n",
      "Losses: train = 0.00600694, test = 0.02703371\n",
      "Test: accuracy = 0.99000743\n",
      "Test: precision = 0.73630144\n",
      "Test: recall = 0.73002084\n",
      "Test: f1_score = 0.72504186\n",
      "new maximum score 0.99000743\n",
      "Saving state_dict... done.\n",
      "Epoch 24: 100%|██████████| 48814/48814 [04:32<00:00, 179.24it/s, train_loss=0.0062] \n",
      "Epoch 24: \n",
      "Losses: train = 0.00590567, test = 0.02691452\n",
      "Test: accuracy = 0.99001587\n",
      "Test: precision = 0.74938951\n",
      "Test: recall = 0.73781009\n",
      "Test: f1_score = 0.73534256\n",
      "new maximum score 0.99001587\n",
      "Saving state_dict... done.\n",
      "Epoch 25: 100%|██████████| 48814/48814 [04:33<00:00, 178.56it/s, train_loss=0.00608]\n",
      "Epoch 25: \n",
      "Losses: train = 0.00575782, test = 0.02741891\n",
      "Test: accuracy = 0.99011714\n",
      "Test: precision = 0.75256423\n",
      "Test: recall = 0.73625267\n",
      "Test: f1_score = 0.73574537\n",
      "new maximum score 0.99011714\n",
      "Saving state_dict... done.\n",
      "Epoch 26: 100%|██████████| 48814/48814 [04:34<00:00, 177.74it/s, train_loss=0.00602]\n",
      "Epoch 26: \n",
      "Losses: train = 0.00560149, test = 0.02637674\n",
      "Test: accuracy = 0.99024374\n",
      "Test: precision = 0.75491148\n",
      "Test: recall = 0.74429117\n",
      "Test: f1_score = 0.74193282\n",
      "new maximum score 0.99024374\n",
      "Saving state_dict... done.\n",
      "Epoch 27: 100%|██████████| 48814/48814 [04:34<00:00, 178.01it/s, train_loss=0.00557]\n",
      "Epoch 27: \n",
      "Losses: train = 0.00539859, test = 0.02709414\n",
      "Test: accuracy = 0.99031126\n",
      "Test: precision = 0.75083495\n",
      "Test: recall = 0.73573109\n",
      "Test: f1_score = 0.73509861\n",
      "new maximum score 0.99031126\n",
      "Saving state_dict... done.\n",
      "Epoch 28: 100%|██████████| 48814/48814 [04:34<00:00, 177.74it/s, train_loss=0.00531]\n",
      "Epoch 28: \n",
      "Losses: train = 0.00523216, test = 0.02731326\n",
      "Test: accuracy = 0.99018466\n",
      "Test: precision = 0.73897751\n",
      "Test: recall = 0.72436687\n",
      "Test: f1_score = 0.72291957\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 29: 100%|██████████| 48814/48814 [04:34<00:00, 177.84it/s, train_loss=0.00545]\n",
      "Epoch 29: \n",
      "Losses: train = 0.00502872, test = 0.02752531\n",
      "Test: accuracy = 0.99015934\n",
      "Test: precision = 0.74897488\n",
      "Test: recall = 0.73558422\n",
      "Test: f1_score = 0.73306404\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 30: 100%|██████████| 48814/48814 [04:35<00:00, 177.30it/s, train_loss=0.00484]\n",
      "Epoch 30: \n",
      "Losses: train = 0.00484735, test = 0.02751293\n",
      "Test: accuracy = 0.99009182\n",
      "Test: precision = 0.74545286\n",
      "Test: recall = 0.73095787\n",
      "Test: f1_score = 0.73024923\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 31: 100%|██████████| 48814/48814 [04:34<00:00, 177.59it/s, train_loss=0.00514]\n",
      "Epoch 31: \n",
      "Losses: train = 0.00475538, test = 0.02757335\n",
      "Test: accuracy = 0.99034501\n",
      "Test: precision = 0.76307997\n",
      "Test: recall = 0.74914716\n",
      "Test: f1_score = 0.74806612\n",
      "new maximum score 0.99034501\n",
      "Saving state_dict... done.\n",
      "Epoch 32: 100%|██████████| 48814/48814 [04:34<00:00, 177.57it/s, train_loss=0.00448]\n",
      "Epoch 32: \n",
      "Losses: train = 0.00455149, test = 0.02772356\n",
      "Test: accuracy = 0.99029438\n",
      "Test: precision = 0.75471521\n",
      "Test: recall = 0.73955760\n",
      "Test: f1_score = 0.73843796\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 33: 100%|██████████| 48814/48814 [04:34<00:00, 177.51it/s, train_loss=0.00459]\n",
      "Epoch 33: \n",
      "Losses: train = 0.00448578, test = 0.02828153\n",
      "Test: accuracy = 0.99026062\n",
      "Test: precision = 0.74449645\n",
      "Test: recall = 0.73493562\n",
      "Test: f1_score = 0.73212642\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 34: 100%|██████████| 48814/48814 [04:35<00:00, 177.20it/s, train_loss=0.00464]\n",
      "Epoch 34: \n",
      "Losses: train = 0.00443691, test = 0.02768194\n",
      "Test: accuracy = 0.99031969\n",
      "Test: precision = 0.75509362\n",
      "Test: recall = 0.74258966\n",
      "Test: f1_score = 0.74014724\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 35: 100%|██████████| 48814/48814 [04:34<00:00, 177.81it/s, train_loss=0.00443]\n",
      "Epoch 35: \n",
      "Losses: train = 0.00422225, test = 0.02846507\n",
      "Test: accuracy = 0.99039565\n",
      "Test: precision = 0.75770236\n",
      "Test: recall = 0.74698210\n",
      "Test: f1_score = 0.74448617\n",
      "new maximum score 0.99039565\n",
      "Saving state_dict... done.\n",
      "Epoch 36: 100%|██████████| 48814/48814 [04:35<00:00, 177.40it/s, train_loss=0.00425]\n",
      "Epoch 36: \n",
      "Losses: train = 0.00406150, test = 0.02947787\n",
      "Test: accuracy = 0.99025218\n",
      "Test: precision = 0.75925847\n",
      "Test: recall = 0.74606925\n",
      "Test: f1_score = 0.74417772\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 37: 100%|██████████| 48814/48814 [04:33<00:00, 178.40it/s, train_loss=0.00416]\n",
      "Epoch 37: \n",
      "Losses: train = 0.00396311, test = 0.03014765\n",
      "Test: accuracy = 0.99036189\n",
      "Test: precision = 0.76111687\n",
      "Test: recall = 0.74988061\n",
      "Test: f1_score = 0.74533009\n",
      "BAD EPOCHS: 1 (<< >)\n",
      "Epoch 38: 100%|██████████| 48814/48814 [04:33<00:00, 178.53it/s, train_loss=0.00423]\n",
      "Epoch 38: \n",
      "Losses: train = 0.00382989, test = 0.02935553\n",
      "Test: accuracy = 0.99010026\n",
      "Test: precision = 0.75461192\n",
      "Test: recall = 0.74541199\n",
      "Test: f1_score = 0.74148354\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 39: 100%|██████████| 48814/48814 [04:35<00:00, 177.34it/s, train_loss=0.00387]\n",
      "Epoch 39: \n",
      "Losses: train = 0.00365360, test = 0.02928955\n",
      "Test: accuracy = 0.99018466\n",
      "Test: precision = 0.74467087\n",
      "Test: recall = 0.73651648\n",
      "Test: f1_score = 0.73115088\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 40: 100%|██████████| 48814/48814 [04:33<00:00, 178.67it/s, train_loss=0.00384]\n",
      "Epoch 40: \n",
      "Losses: train = 0.00360677, test = 0.03007255\n",
      "Test: accuracy = 0.99040409\n",
      "Test: precision = 0.76212166\n",
      "Test: recall = 0.74763680\n",
      "Test: f1_score = 0.74531401\n",
      "new maximum score 0.99040409\n",
      "Saving state_dict... done.\n",
      "Epoch 41: 100%|██████████| 48814/48814 [04:32<00:00, 179.04it/s, train_loss=0.00363]\n",
      "Epoch 41: \n",
      "Losses: train = 0.00343372, test = 0.02967071\n",
      "Test: accuracy = 0.99034501\n",
      "Test: precision = 0.75432545\n",
      "Test: recall = 0.74589705\n",
      "Test: f1_score = 0.74264113\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 42: 100%|██████████| 48814/48814 [04:32<00:00, 178.93it/s, train_loss=0.00378]\n",
      "Epoch 42: \n",
      "Losses: train = 0.00338626, test = 0.03010640\n",
      "Test: accuracy = 0.99034501\n",
      "Test: precision = 0.75705047\n",
      "Test: recall = 0.74469368\n",
      "Test: f1_score = 0.74312402\n",
      "BAD EPOCHS: 2 (<< =)\n",
      "Epoch 43: 100%|██████████| 48814/48814 [04:33<00:00, 178.60it/s, train_loss=0.00337]\n",
      "Epoch 43: \n",
      "Losses: train = 0.00314697, test = 0.03097975\n",
      "Test: accuracy = 0.99045473\n",
      "Test: precision = 0.76582417\n",
      "Test: recall = 0.75208502\n",
      "Test: f1_score = 0.75032583\n",
      "new maximum score 0.99045473\n",
      "Saving state_dict... done.\n",
      "Epoch 44: 100%|██████████| 48814/48814 [04:33<00:00, 178.81it/s, train_loss=0.00336]\n",
      "Epoch 44: \n",
      "Losses: train = 0.00314668, test = 0.03089498\n",
      "Test: accuracy = 0.99036189\n",
      "Test: precision = 0.75267625\n",
      "Test: recall = 0.74122119\n",
      "Test: f1_score = 0.73888059\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 45: 100%|██████████| 48814/48814 [04:32<00:00, 178.98it/s, train_loss=0.0036] \n",
      "Epoch 45: \n",
      "Losses: train = 0.00308124, test = 0.03143364\n",
      "Test: accuracy = 0.99016778\n",
      "Test: precision = 0.74400381\n",
      "Test: recall = 0.73580129\n",
      "Test: f1_score = 0.73111104\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 46: 100%|██████████| 48814/48814 [04:33<00:00, 178.53it/s, train_loss=0.00336]\n",
      "Epoch 46: \n",
      "Losses: train = 0.00299404, test = 0.03136385\n",
      "Test: accuracy = 0.99037877\n",
      "Test: precision = 0.74969878\n",
      "Test: recall = 0.73754395\n",
      "Test: f1_score = 0.73504220\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 47: 100%|██████████| 48814/48814 [04:32<00:00, 178.90it/s, train_loss=0.00293]\n",
      "Epoch 47: \n",
      "Losses: train = 0.00282500, test = 0.03254269\n",
      "Test: accuracy = 0.99025218\n",
      "Test: precision = 0.74917858\n",
      "Test: recall = 0.73786038\n",
      "Test: f1_score = 0.73515123\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 48: 100%|██████████| 48814/48814 [04:32<00:00, 179.05it/s, train_loss=0.00292]\n",
      "Epoch 48: \n",
      "Losses: train = 0.00269764, test = 0.03192836\n",
      "Test: accuracy = 0.99023530\n",
      "Test: precision = 0.74849359\n",
      "Test: recall = 0.73317446\n",
      "Test: f1_score = 0.73101047\n",
      "BAD EPOCHS: 4 (<< <)\n",
      "Epoch 49: 100%|██████████| 48814/48814 [04:33<00:00, 178.35it/s, train_loss=0.0028] \n",
      "Epoch 49: \n",
      "Losses: train = 0.00256751, test = 0.03256299\n",
      "Test: accuracy = 0.99032813\n",
      "Test: precision = 0.75675282\n",
      "Test: recall = 0.73531784\n",
      "Test: f1_score = 0.73793738\n",
      "BAD EPOCHS: 4 (<< >)\n",
      "Epoch 50: 100%|██████████| 48814/48814 [04:32<00:00, 179.12it/s, train_loss=0.00268]\n",
      "Epoch 50: \n",
      "Losses: train = 0.00257463, test = 0.03259697\n",
      "Test: accuracy = 0.99037877\n",
      "Test: precision = 0.75595922\n",
      "Test: recall = 0.73770024\n",
      "Test: f1_score = 0.73845103\n",
      "BAD EPOCHS: 4 (<< >)\n",
      "Epoch 51: 100%|██████████| 48814/48814 [04:33<00:00, 178.53it/s, train_loss=0.00246]\n",
      "Epoch 51: \n",
      "Losses: train = 0.00234179, test = 0.03306224\n",
      "Test: accuracy = 0.99026062\n",
      "Test: precision = 0.74164733\n",
      "Test: recall = 0.73109162\n",
      "Test: f1_score = 0.72825156\n",
      "BAD EPOCHS: 5 (<< <)\n",
      "Maximum bad epochs exceeded. Process has been stopped. Best score 0.9904547295928702 (on epoch 43)\n",
      "\n",
      "MODEL TUNING\n",
      "Loading state_dict... done.\n",
      "Epoch 1: 100%|██████████| 48814/48814 [04:29<00:00, 181.05it/s, train_loss=0.00276]\n",
      "Epoch 1: \n",
      "Losses: train = 0.00279187, test = 0.02976739\n",
      "Test: accuracy = 0.99045473\n",
      "Test: precision = 0.76812578\n",
      "Test: recall = 0.75376013\n",
      "Test: f1_score = 0.75249066\n",
      "BAD EPOCHS: 1 (== =)\n",
      "Epoch 2: 100%|██████████| 48814/48814 [04:29<00:00, 181.09it/s, train_loss=0.00283]\n",
      "Epoch 2: \n",
      "Losses: train = 0.00275005, test = 0.03028018\n",
      "Test: accuracy = 0.99048005\n",
      "Test: precision = 0.76740705\n",
      "Test: recall = 0.75397874\n",
      "Test: f1_score = 0.75239629\n",
      "new maximum score 0.99048005\n",
      "Saving state_dict... done.\n",
      "Epoch 3: 100%|██████████| 48814/48814 [04:29<00:00, 181.00it/s, train_loss=0.00264]\n",
      "Epoch 3: \n",
      "Losses: train = 0.00271753, test = 0.03029092\n",
      "Test: accuracy = 0.99052225\n",
      "Test: precision = 0.76866310\n",
      "Test: recall = 0.75404215\n",
      "Test: f1_score = 0.75343384\n",
      "new maximum score 0.99052225\n",
      "Saving state_dict... done.\n",
      "Epoch 4: 100%|██████████| 48814/48814 [04:28<00:00, 182.09it/s, train_loss=0.00259]\n",
      "Epoch 4: \n",
      "Losses: train = 0.00269283, test = 0.03041457\n",
      "Test: accuracy = 0.99048849\n",
      "Test: precision = 0.76871710\n",
      "Test: recall = 0.75449482\n",
      "Test: f1_score = 0.75325629\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 5: 100%|██████████| 48814/48814 [04:29<00:00, 181.45it/s, train_loss=0.00259]\n",
      "Epoch 5: \n",
      "Losses: train = 0.00268273, test = 0.03037327\n",
      "Test: accuracy = 0.99057289\n",
      "Test: precision = 0.76958667\n",
      "Test: recall = 0.75524262\n",
      "Test: f1_score = 0.75446578\n",
      "new maximum score 0.99057289\n",
      "Saving state_dict... done.\n",
      "Epoch 6: 100%|██████████| 48814/48814 [04:29<00:00, 181.41it/s, train_loss=0.00252]\n",
      "Epoch 6: \n",
      "Losses: train = 0.00260388, test = 0.03060484\n",
      "Test: accuracy = 0.99052225\n",
      "Test: precision = 0.76686958\n",
      "Test: recall = 0.75307266\n",
      "Test: f1_score = 0.75192701\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 7: 100%|██████████| 48814/48814 [04:30<00:00, 180.75it/s, train_loss=0.00272]\n",
      "Epoch 7: \n",
      "Losses: train = 0.00268258, test = 0.03027812\n",
      "Test: accuracy = 0.99051381\n",
      "Test: precision = 0.76774933\n",
      "Test: recall = 0.75428183\n",
      "Test: f1_score = 0.75289057\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 8: 100%|██████████| 48814/48814 [04:29<00:00, 181.30it/s, train_loss=0.00252]\n",
      "Epoch 8: \n",
      "Losses: train = 0.00253635, test = 0.03042394\n",
      "Test: accuracy = 0.99053913\n",
      "Test: precision = 0.76704793\n",
      "Test: recall = 0.75510694\n",
      "Test: f1_score = 0.75330344\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 9: 100%|██████████| 48814/48814 [04:28<00:00, 181.55it/s, train_loss=0.0026] \n",
      "Epoch 9: \n",
      "Losses: train = 0.00254023, test = 0.03037294\n",
      "Test: accuracy = 0.99049693\n",
      "Test: precision = 0.76418015\n",
      "Test: recall = 0.75277690\n",
      "Test: f1_score = 0.75130261\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 10: 100%|██████████| 48814/48814 [04:29<00:00, 181.15it/s, train_loss=0.00253]\n",
      "Epoch 10: \n",
      "Losses: train = 0.00254097, test = 0.03045428\n",
      "Test: accuracy = 0.99055601\n",
      "Test: precision = 0.76964445\n",
      "Test: recall = 0.75725285\n",
      "Test: f1_score = 0.75609375\n",
      "BAD EPOCHS: 3 (<< >)\n",
      "Epoch 11: 100%|██████████| 48814/48814 [04:30<00:00, 180.66it/s, train_loss=0.00261]\n",
      "Epoch 11: \n",
      "Losses: train = 0.00251164, test = 0.02992616\n",
      "Test: accuracy = 0.99053913\n",
      "Test: precision = 0.76954923\n",
      "Test: recall = 0.75816864\n",
      "Test: f1_score = 0.75636763\n",
      "BAD EPOCHS: 4 (<< <)\n",
      "Epoch 12: 100%|██████████| 48814/48814 [04:28<00:00, 181.55it/s, train_loss=0.00268]\n",
      "Epoch 12: \n",
      "Losses: train = 0.00258765, test = 0.02996741\n",
      "Test: accuracy = 0.99061508\n",
      "Test: precision = 0.76805994\n",
      "Test: recall = 0.75557989\n",
      "Test: f1_score = 0.75450148\n",
      "new maximum score 0.99061508\n",
      "Saving state_dict... done.\n",
      "Epoch 13: 100%|██████████| 48814/48814 [04:29<00:00, 180.96it/s, train_loss=0.00248]\n",
      "Epoch 13: \n",
      "Losses: train = 0.00245701, test = 0.03082598\n",
      "Test: accuracy = 0.99061508\n",
      "Test: precision = 0.76943528\n",
      "Test: recall = 0.75627354\n",
      "Test: f1_score = 0.75503514\n",
      "BAD EPOCHS: 1 (== =)\n",
      "Epoch 14: 100%|██████████| 48814/48814 [04:29<00:00, 181.20it/s, train_loss=0.00244]\n",
      "Epoch 14: \n",
      "Losses: train = 0.00249350, test = 0.03069945\n",
      "Test: accuracy = 0.99064040\n",
      "Test: precision = 0.76963965\n",
      "Test: recall = 0.75715304\n",
      "Test: f1_score = 0.75610961\n",
      "new maximum score 0.99064040\n",
      "Saving state_dict... done.\n",
      "Epoch 15: 100%|██████████| 48814/48814 [04:27<00:00, 182.20it/s, train_loss=0.00231]\n",
      "Epoch 15: \n",
      "Losses: train = 0.00249222, test = 0.03033109\n",
      "Test: accuracy = 0.99059820\n",
      "Test: precision = 0.77016281\n",
      "Test: recall = 0.75793328\n",
      "Test: f1_score = 0.75683541\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 16: 100%|██████████| 48814/48814 [04:30<00:00, 180.58it/s, train_loss=0.00263]\n",
      "Epoch 16: \n",
      "Losses: train = 0.00249328, test = 0.02986106\n",
      "Test: accuracy = 0.99057289\n",
      "Test: precision = 0.76664196\n",
      "Test: recall = 0.75527814\n",
      "Test: f1_score = 0.75350408\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 17: 100%|██████████| 48814/48814 [04:29<00:00, 181.35it/s, train_loss=0.00245]\n",
      "Epoch 17: \n",
      "Losses: train = 0.00240085, test = 0.03062938\n",
      "Test: accuracy = 0.99063196\n",
      "Test: precision = 0.76663343\n",
      "Test: recall = 0.75359404\n",
      "Test: f1_score = 0.75293621\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 18: 100%|██████████| 48814/48814 [04:28<00:00, 181.64it/s, train_loss=0.0025] \n",
      "Epoch 18: \n",
      "Losses: train = 0.00243445, test = 0.03057378\n",
      "Test: accuracy = 0.99059820\n",
      "Test: precision = 0.76958267\n",
      "Test: recall = 0.75727442\n",
      "Test: f1_score = 0.75547448\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 19: 100%|██████████| 48814/48814 [04:29<00:00, 181.37it/s, train_loss=0.00261]\n",
      "Epoch 19: \n",
      "Losses: train = 0.00249186, test = 0.02999157\n",
      "Test: accuracy = 0.99053913\n",
      "Test: precision = 0.76827027\n",
      "Test: recall = 0.75621166\n",
      "Test: f1_score = 0.75491234\n",
      "BAD EPOCHS: 4 (<< <)\n",
      "Epoch 20: 100%|██████████| 48814/48814 [04:29<00:00, 180.98it/s, train_loss=0.00248]\n",
      "Epoch 20: \n",
      "Losses: train = 0.00236130, test = 0.03084320\n",
      "Test: accuracy = 0.99061508\n",
      "Test: precision = 0.77168709\n",
      "Test: recall = 0.75795240\n",
      "Test: f1_score = 0.75705529\n",
      "BAD EPOCHS: 4 (<< >)\n",
      "Epoch 21: 100%|██████████| 48814/48814 [04:29<00:00, 181.33it/s, train_loss=0.0024] \n",
      "Epoch 21: \n",
      "Losses: train = 0.00244231, test = 0.03036896\n",
      "Test: accuracy = 0.99055601\n",
      "Test: precision = 0.76566889\n",
      "Test: recall = 0.75285108\n",
      "Test: f1_score = 0.75218291\n",
      "BAD EPOCHS: 5 (<< <)\n",
      "Maximum bad epochs exceeded. Process has been stopped. Best score 0.990640402403619 (on epoch 14)\n",
      "\n",
      "=== LEMMA TAGGER TRAINING HAS FINISHED ===\n",
      "\n",
      "Use the `.load('lemma_model')` method to start working with the LEMMA tagger.\n"
     ]
    }
   ],
   "source": [
    "tagger = LemmaTagger()\n",
    "tagger.load_train_corpus(corpus.train)\n",
    "tagger.load_test_corpus(corpus.dev)\n",
    "\n",
    "_ = tagger.train(MODEL_FN, device='cuda:8', word_emb_type='bert',\n",
    "                 word_emb_path=MODEL_FN.replace('model', BERT_MODEL_FN)\n",
    "                             + '_len512_ep3_bat8_seed42',\n",
    "                 word_emb_tune_params={\n",
    "                     'model_name': BERT_MODEL_FN,\n",
    "                     'max_len': 512, 'epochs': 3, 'batch_size': 8\n",
    "                 },\n",
    "                 rnn_emb_dim=None, cnn_emb_dim=None, cnn_kernels=range(1, 7),\n",
    "                 upos_emb_dim=300, lstm_layers=3, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... done.\n",
      "Creating model... done.\n",
      "Loading state_dict... done.\n",
      "Fit corpus dict... done.\n"
     ]
    }
   ],
   "source": [
    "res_dev = 'corpora/_dev_' + MODEL_FN.replace('_model', '.conllu')\n",
    "res_test = 'corpora/_test_' + MODEL_FN.replace('_model', '.conllu')\n",
    "tagger = LemmaTagger()\n",
    "tagger.load(MODEL_FN)\n",
    "junky.clear_tqdm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing\n",
      "100%|██████████| 103/103 [00:10<00:00,  9.56it/s]\n",
      "Adjusting\n",
      "100%|██████████| 6584/6584 [00:23<00:00, 282.71it/s]\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.predict(corpus.dev, clone_ds=True, save_to=res_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LEMMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus\n",
      "100%|██████████| 6584/6584 [01:09<00:00, 94.38it/s] \n",
      "LEMMA total: 118488\n",
      "    correct: 117459\n",
      "      wrong: 1029\n",
      "   Accuracy: 0.9913155762608872\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LEMMA\n",
      "Load corpus\n",
      "[> 900                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[> 900                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=> 1600                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=> 1600                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===> 3100                                                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[===> 3100                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====> 5000                                                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[====> 5000                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======> 6100                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[======> 6100                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n",
      "LEMMA total: 118488\n",
      "    correct: 117459\n",
      "      wrong: 1029\n",
      "   Accuracy: 0.9913155762608872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.dev, res_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus\n",
      "100%|██████████| 6491/6491 [01:08<00:00, 95.04it/s] \n"
     ]
    }
   ],
   "source": [
    "_ = tagger.predict(corpus.test, save_to=res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LEMMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing\n",
      "100%|██████████| 102/102 [00:10<00:00,  9.65it/s]\n",
      "Adjusting\n",
      "100%|██████████| 6491/6491 [00:23<00:00, 278.95it/s]\n",
      "LEMMA total: 117329\n",
      "    correct: 116310\n",
      "      wrong: 1019\n",
      "   Accuracy: 0.9913150201569945\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.test, clone_ds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LEMMA\n",
      "Load corpus\n",
      "[> 700                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[> 700                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=> 1300                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=> 1300                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==> 2300                                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[==> 2200                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===> 3100                                                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[==> 3000                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====> 4700                                                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[====> 4700                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====> 5800                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=====> 5800                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n",
      "LEMMA total: 117329\n",
      "    correct: 116310\n",
      "      wrong: 1019\n",
      "   Accuracy: 0.9913150201569945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus.test, res_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL18 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading UniversalDependencies/UD_Russian-SynTagRus contents\n",
      "done: 9525 bytes\n"
     ]
    }
   ],
   "source": [
    "from corpuscula.corpus_utils import download_ud, get_ud_test_path\n",
    "import junky\n",
    "from mordl import UposTagger, FeatsTagger, LemmaTagger, conll18_ud_eval\n",
    "\n",
    "# we use UD Taiga corpus only as example. For real model training comment\n",
    "# Taiga and uncomment SynTagRus\n",
    "#corpus_name = 'UD_Russian-Taiga'\n",
    "corpus_name = 'UD_Russian-SynTagRus'\n",
    "download_ud(corpus_name, overwrite=False)\n",
    "\n",
    "corpus_gold = get_ud_test_path(corpus_name)\n",
    "corpus_test = 'corpora/_test_tagged.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... done.\n",
      "Creating model... done.\n",
      "Loading state_dict... done.\n",
      "Fit corpus dict... done.\n",
      "Loading dataset... done.\n",
      "Creating model... done.\n",
      "Loading state_dict... done.\n",
      "Fit corpus dict... done.\n",
      "Loading dataset... done.\n",
      "Creating model... done.\n",
      "Loading state_dict... done.\n",
      "Fit corpus dict... done.\n"
     ]
    }
   ],
   "source": [
    "tagger_u = UposTagger()\n",
    "tagger_u.load('upos_model', device='cuda:11', dataset_device='cuda:11')\n",
    "tagger_f = FeatsTagger()\n",
    "tagger_f.load('feats_model', device='cuda:11', dataset_device='cuda:11')\n",
    "tagger_l = LemmaTagger()\n",
    "tagger_l.load('lemma_model', device='cuda:11', dataset_device='cuda:11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n",
      "Processing corpus\n",
      "100%|██████████| 6491/6491 [01:07<00:00, 96.49it/s] \n",
      "Processing corpus\n",
      "100%|██████████| 6491/6491 [01:07<00:00, 95.95it/s] \n",
      "Processing corpus\n",
      "100%|██████████| 6491/6491 [01:08<00:00, 94.37it/s] \n"
     ]
    }
   ],
   "source": [
    "_ = tagger_l.predict(\n",
    "    tagger_f.predict(\n",
    "        tagger_u.predict(corpus_gold)\n",
    "    ), save_to=corpus_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |    100.00 |    100.00 |    100.00 |\n",
      "Sentences  |    100.00 |    100.00 |    100.00 |\n",
      "Words      |    100.00 |    100.00 |    100.00 |\n",
      "UPOS       |     99.15 |     99.15 |     99.15 |     99.15\n",
      "XPOS       |    100.00 |    100.00 |    100.00 |    100.00\n",
      "UFeats     |     97.75 |     97.75 |     97.75 |     97.75\n",
      "AllTags    |     97.55 |     97.55 |     97.55 |     97.55\n",
      "Lemmas     |     98.57 |     98.57 |     98.57 |     98.57\n",
      "UAS        |    100.00 |    100.00 |    100.00 |    100.00\n",
      "LAS        |    100.00 |    100.00 |    100.00 |    100.00\n",
      "CLAS       |    100.00 |    100.00 |    100.00 |    100.00\n",
      "MLAS       |     96.22 |     96.22 |     96.22 |     96.22\n",
      "BLEX       |     97.83 |     97.83 |     97.83 |     97.83\n"
     ]
    }
   ],
   "source": [
    "conll18_ud_eval(corpus_gold, corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISC:NE\n",
    "\n",
    "Note: The corpora we used are proprietary. You have to find another corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... done.\n",
      "Creating model... done.\n",
      "Loading state_dict... done.\n",
      "Fit corpus dict... done.\n",
      "Loading dataset... done.\n",
      "Creating model... done.\n",
      "Loading state_dict... done.\n",
      "Fit corpus dict... done.\n",
      "Load corpus\n",
      "[===============================] 30390                             \n",
      "Corpus has been loaded: 30390 sentences, 378829 tokens\n",
      "Processing corpus\n",
      "100%|██████████| 30390/30390 [03:54<00:00, 129.38it/s]\n",
      "Processing corpus\n",
      "100%|██████████| 30390/30390 [03:55<00:00, 128.91it/s]\n",
      "Load corpus\n",
      "[====] 3799                                                        \n",
      "Corpus has been loaded: 3799 sentences, 47280 tokens\n",
      "Processing corpus\n",
      "100%|██████████| 3799/3799 [00:29<00:00, 128.57it/s]\n",
      "Processing corpus\n",
      "100%|██████████| 3799/3799 [00:29<00:00, 128.80it/s]\n",
      "Load corpus\n",
      "[====] 3798                                                        \n",
      "Corpus has been loaded: 3798 sentences, 47126 tokens\n",
      "Processing corpus\n",
      "100%|██████████| 3798/3798 [00:29<00:00, 129.88it/s]\n",
      "Processing corpus\n",
      "100%|██████████| 3798/3798 [00:29<00:00, 127.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from mordl import UposTagger, FeatsTagger\n",
    "\n",
    "tagger_u = UposTagger()\n",
    "tagger_u.load('upos_model')\n",
    "tagger_f = FeatsTagger()\n",
    "tagger_f.load('feats_model')\n",
    "\n",
    "for corpora in zip(['corpora/ner_train.conllu',\n",
    "                    'corpora/ner_dev.conllu',\n",
    "                    'corpora/ner_test.conllu'],\n",
    "                   ['corpora/ner_train_upos_feats.conllu',\n",
    "                    'corpora/ner_dev_upos_feats.conllu',\n",
    "                    'corpora/ner_test_upos_feats.conllu']):\n",
    "    tagger_f.predict(\n",
    "        tagger_u.predict(corpora[0]), save_to=corpora[1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import junky\n",
    "from mordl import NeTagger\n",
    "\n",
    "BERT_MODEL_FN = 'bert-base-multilingual-cased'\n",
    "MODEL_FN = 'misc-ne_model'\n",
    "SEED=42\n",
    "\n",
    "corpus_train = 'corpora/ner_train_upos_feats.conllu'\n",
    "corpus_dev = 'corpora/ner_dev_upos_feats.conllu'\n",
    "corpus_test = 'corpora/ner_test_upos_feats.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[===============================] 30390                             \n",
      "Corpus has been loaded: 30390 sentences, 378829 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[====] 3799                                                        \n",
      "Corpus has been loaded: 3799 sentences, 47280 tokens\n",
      "Parse corpus\n",
      "[###############################] 100%                             \n",
      "done: 30390 sentences, 362786 acceptable tokens (plus 0 for YO letters)\n",
      "Fit corpus dict... done.\n",
      "=== MISC:NE TAGGER TRAINING PIPELINE ===\n",
      "\n",
      "BERT MODEL TUNING 'bert-base-multilingual-cased'. The result's model name will be 'misc-ne_bert-base-multilingual-cased_len512_ep3_bat8_seed42'\n",
      "Loading tokenizer...\n",
      "Tokenizer is loaded. Vocab size: 119547\n",
      "Corpora processing... done.\n",
      "Loading model 'bert-base-multilingual-cased'... done.\n",
      "Epoch 1: 100%|██████████| 30390/30390 [07:06<00:00, 71.19it/s, train_loss=0.506] \n",
      "Average train loss: 0.22115723773073204\n",
      "Dev: accuracy = 0.94016071\n",
      "Dev: precision = 0.64285216\n",
      "Dev: recall = 0.60400200\n",
      "Dev: f1_score = 0.60815842\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to misc-ne_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "Epoch 2: 100%|██████████| 30390/30390 [07:06<00:00, 71.24it/s, train_loss=0.113]  \n",
      "Average train loss: 0.13452649930620503\n",
      "Dev: accuracy = 0.94571112\n",
      "Dev: precision = 0.67449232\n",
      "Dev: recall = 0.65385552\n",
      "Dev: f1_score = 0.65974808\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to misc-ne_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "Epoch 3: 100%|██████████| 30390/30390 [07:05<00:00, 71.46it/s, train_loss=0.119]  \n",
      "Average train loss: 0.0896721976198414\n",
      "Dev: accuracy = 0.94637318\n",
      "Dev: precision = 0.71431601\n",
      "Dev: recall = 0.74003630\n",
      "Dev: f1_score = 0.71776732\n",
      "NB: Scores may be high because of tags stretching\n",
      "Saving model to misc-ne_bert-base-multilingual-cased_len512_ep3_bat8_seed42\n",
      "\n",
      "DATASETS CREATION\n",
      "Tokenizing\n",
      "100%|██████████| 30390/30390 [00:14<00:00, 2054.11it/s]\n",
      "Vectorizing\n",
      "100%|██████████| 475/475 [00:43<00:00, 10.97it/s]\n",
      "Adjusting\n",
      "100%|██████████| 30390/30390 [01:20<00:00, 379.29it/s]\n",
      "\n",
      "MODEL CREATION\n",
      "Config saved\n",
      "\n",
      "MODEL TRAINING\n",
      "Epoch 1: 100%|██████████| 30390/30390 [01:19<00:00, 378.21it/s, train_loss=0.0405]\n",
      "Epoch 1: \n",
      "Losses: train = 0.05165659, test = 0.08865500\n",
      "Test: accuracy = 0.94223773\n",
      "Test: precision = 0.65742239\n",
      "Test: recall = 0.60904735\n",
      "Test: f1_score = 0.62747601\n",
      "new maximum score 0.94223773\n",
      "Saving state_dict... done.\n",
      "Epoch 2: 100%|██████████| 30390/30390 [01:21<00:00, 375.02it/s, train_loss=0.0345]\n",
      "Epoch 2: \n",
      "Losses: train = 0.03616919, test = 0.08817365\n",
      "Test: accuracy = 0.94194162\n",
      "Test: precision = 0.69719845\n",
      "Test: recall = 0.63299420\n",
      "Test: f1_score = 0.65853400\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 3: 100%|██████████| 30390/30390 [01:20<00:00, 358.73it/s, train_loss=0.034] \n",
      "Epoch 3: \n",
      "Losses: train = 0.03430944, test = 0.08547696\n",
      "Test: accuracy = 0.94236464\n",
      "Test: precision = 0.74812206\n",
      "Test: recall = 0.64371807\n",
      "Test: f1_score = 0.67962411\n",
      "new maximum score 0.94236464\n",
      "Saving state_dict... done.\n",
      "Epoch 4: 100%|██████████| 30390/30390 [01:21<00:00, 387.99it/s, train_loss=0.0334]\n",
      "Epoch 4: \n",
      "Losses: train = 0.03354067, test = 0.08342817\n",
      "Test: accuracy = 0.94242809\n",
      "Test: precision = 0.73711499\n",
      "Test: recall = 0.70844066\n",
      "Test: f1_score = 0.71619901\n",
      "new maximum score 0.94242809\n",
      "Saving state_dict... done.\n",
      "Epoch 5: 100%|██████████| 30390/30390 [01:20<00:00, 318.88it/s, train_loss=0.0326]\n",
      "Epoch 5: \n",
      "Losses: train = 0.03235944, test = 0.08551712\n",
      "Test: accuracy = 0.94261844\n",
      "Test: precision = 0.75292476\n",
      "Test: recall = 0.65358821\n",
      "Test: f1_score = 0.68977118\n",
      "new maximum score 0.94261844\n",
      "Saving state_dict... done.\n",
      "Epoch 6: 100%|██████████| 30390/30390 [01:20<00:00, 363.21it/s, train_loss=0.0322]\n",
      "Epoch 6: \n",
      "Losses: train = 0.03191439, test = 0.08532209\n",
      "Test: accuracy = 0.94202623\n",
      "Test: precision = 0.75339053\n",
      "Test: recall = 0.67068856\n",
      "Test: f1_score = 0.70216554\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 7: 100%|██████████| 30390/30390 [01:21<00:00, 374.22it/s, train_loss=0.0315]\n",
      "Epoch 7: \n",
      "Losses: train = 0.03123138, test = 0.08579142\n",
      "Test: accuracy = 0.94230118\n",
      "Test: precision = 0.76192401\n",
      "Test: recall = 0.65189884\n",
      "Test: f1_score = 0.68332311\n",
      "BAD EPOCHS: 1 (<< >)\n",
      "Epoch 8: 100%|██████████| 30390/30390 [01:21<00:00, 343.72it/s, train_loss=0.0306]\n",
      "Epoch 8: \n",
      "Losses: train = 0.03113311, test = 0.08635695\n",
      "Test: accuracy = 0.94192047\n",
      "Test: precision = 0.76443703\n",
      "Test: recall = 0.63902546\n",
      "Test: f1_score = 0.66841449\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 9: 100%|██████████| 30390/30390 [01:20<00:00, 378.64it/s, train_loss=0.0313]\n",
      "Epoch 9: \n",
      "Losses: train = 0.03101511, test = 0.08436991\n",
      "Test: accuracy = 0.94170897\n",
      "Test: precision = 0.73546909\n",
      "Test: recall = 0.67323130\n",
      "Test: f1_score = 0.69866523\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 10: 100%|██████████| 30390/30390 [01:20<00:00, 375.37it/s, train_loss=0.0306]\n",
      "Epoch 10: \n",
      "Losses: train = 0.03014662, test = 0.08316810\n",
      "Test: accuracy = 0.94219543\n",
      "Test: precision = 0.73210929\n",
      "Test: recall = 0.69491086\n",
      "Test: f1_score = 0.70602657\n",
      "BAD EPOCHS: 3 (<< >)\n",
      "Epoch 11: 100%|██████████| 30390/30390 [01:20<00:00, 375.58it/s, train_loss=0.0296]\n",
      "Epoch 11: \n",
      "Losses: train = 0.02990599, test = 0.08516747\n",
      "Test: accuracy = 0.94255499\n",
      "Test: precision = 0.75700373\n",
      "Test: recall = 0.68003916\n",
      "Test: f1_score = 0.70984476\n",
      "BAD EPOCHS: 3 (<< >)\n",
      "Epoch 12: 100%|██████████| 30390/30390 [01:21<00:00, 371.15it/s, train_loss=0.0295]\n",
      "Epoch 12: \n",
      "Losses: train = 0.02963033, test = 0.08411607\n",
      "Test: accuracy = 0.94189932\n",
      "Test: precision = 0.71257509\n",
      "Test: recall = 0.70869808\n",
      "Test: f1_score = 0.70509938\n",
      "BAD EPOCHS: 4 (<< <)\n",
      "Epoch 13: 100%|██████████| 30390/30390 [01:20<00:00, 385.78it/s, train_loss=0.0301]\n",
      "Epoch 13: \n",
      "Losses: train = 0.02971950, test = 0.08464373\n",
      "Test: accuracy = 0.94282995\n",
      "Test: precision = 0.75367600\n",
      "Test: recall = 0.67977911\n",
      "Test: f1_score = 0.70750190\n",
      "new maximum score 0.94282995\n",
      "Saving state_dict... done.\n",
      "Epoch 14: 100%|██████████| 30390/30390 [01:20<00:00, 375.96it/s, train_loss=0.029] \n",
      "Epoch 14: \n",
      "Losses: train = 0.02917346, test = 0.08667422\n",
      "Test: accuracy = 0.94223773\n",
      "Test: precision = 0.71071707\n",
      "Test: recall = 0.69495477\n",
      "Test: f1_score = 0.69777343\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 15: 100%|██████████| 30390/30390 [01:20<00:00, 375.76it/s, train_loss=0.029] \n",
      "Epoch 15: \n",
      "Losses: train = 0.02896635, test = 0.08512974\n",
      "Test: accuracy = 0.94251269\n",
      "Test: precision = 0.73515253\n",
      "Test: recall = 0.68018416\n",
      "Test: f1_score = 0.70189467\n",
      "BAD EPOCHS: 1 (<< >)\n",
      "Epoch 16: 100%|██████████| 30390/30390 [01:21<00:00, 392.82it/s, train_loss=0.029] \n",
      "Epoch 16: \n",
      "Losses: train = 0.02861308, test = 0.08721581\n",
      "Test: accuracy = 0.94266074\n",
      "Test: precision = 0.76425362\n",
      "Test: recall = 0.67204248\n",
      "Test: f1_score = 0.70448009\n",
      "BAD EPOCHS: 1 (<< >)\n",
      "Epoch 17: 100%|██████████| 30390/30390 [01:20<00:00, 380.82it/s, train_loss=0.028] \n",
      "Epoch 17: \n",
      "Losses: train = 0.02817906, test = 0.08848028\n",
      "Test: accuracy = 0.94175127\n",
      "Test: precision = 0.73125828\n",
      "Test: recall = 0.69045889\n",
      "Test: f1_score = 0.70095569\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 18: 100%|██████████| 30390/30390 [01:20<00:00, 375.20it/s, train_loss=0.0283]\n",
      "Epoch 18: \n",
      "Losses: train = 0.02780596, test = 0.08874769\n",
      "Test: accuracy = 0.94213198\n",
      "Test: precision = 0.75647312\n",
      "Test: recall = 0.67090616\n",
      "Test: f1_score = 0.70485774\n",
      "BAD EPOCHS: 2 (<< >)\n",
      "Epoch 19: 100%|██████████| 30390/30390 [01:21<00:00, 328.62it/s, train_loss=0.0278]\n",
      "Epoch 19: \n",
      "Losses: train = 0.02736575, test = 0.08872858\n",
      "Test: accuracy = 0.94213198\n",
      "Test: precision = 0.76509676\n",
      "Test: recall = 0.66895329\n",
      "Test: f1_score = 0.70255041\n",
      "BAD EPOCHS: 3 (<< =)\n",
      "Epoch 20: 100%|██████████| 30390/30390 [01:21<00:00, 354.40it/s, train_loss=0.027] \n",
      "Epoch 20: \n",
      "Losses: train = 0.02682523, test = 0.08907166\n",
      "Test: accuracy = 0.94213198\n",
      "Test: precision = 0.75995694\n",
      "Test: recall = 0.66613720\n",
      "Test: f1_score = 0.69972993\n",
      "BAD EPOCHS: 4 (<< =)\n",
      "Epoch 21: 100%|██████████| 30390/30390 [01:20<00:00, 378.34it/s, train_loss=0.0265]\n",
      "Epoch 21: \n",
      "Losses: train = 0.02641534, test = 0.08938938\n",
      "Test: accuracy = 0.94257614\n",
      "Test: precision = 0.72928974\n",
      "Test: recall = 0.69051863\n",
      "Test: f1_score = 0.70298793\n",
      "BAD EPOCHS: 4 (<< >)\n",
      "Epoch 22: 100%|██████████| 30390/30390 [01:20<00:00, 376.63it/s, train_loss=0.0263]\n",
      "Epoch 22: \n",
      "Losses: train = 0.02624505, test = 0.09060014\n",
      "Test: accuracy = 0.94253384\n",
      "Test: precision = 0.74884735\n",
      "Test: recall = 0.68584862\n",
      "Test: f1_score = 0.71250440\n",
      "BAD EPOCHS: 5 (<< <)\n",
      "Maximum bad epochs exceeded. Process has been stopped. Best score 0.9428299492385787 (on epoch 13)\n",
      "\n",
      "MODEL TUNING\n",
      "Loading state_dict... done.\n",
      "Epoch 1: 100%|██████████| 30390/30390 [01:19<00:00, 382.30it/s, train_loss=0.028] \n",
      "Epoch 1: \n",
      "Losses: train = 0.02824135, test = 0.08540470\n",
      "Test: accuracy = 0.94282995\n",
      "Test: precision = 0.76460900\n",
      "Test: recall = 0.67037687\n",
      "Test: f1_score = 0.70263732\n",
      "BAD EPOCHS: 1 (== =)\n",
      "Epoch 2: 100%|██████████| 30390/30390 [01:19<00:00, 394.04it/s, train_loss=0.0286]\n",
      "Epoch 2: \n",
      "Losses: train = 0.02841591, test = 0.08483216\n",
      "Test: accuracy = 0.94282995\n",
      "Test: precision = 0.74691881\n",
      "Test: recall = 0.68388414\n",
      "Test: f1_score = 0.70900186\n",
      "BAD EPOCHS: 2 (== =)\n",
      "Epoch 3: 100%|██████████| 30390/30390 [01:18<00:00, 397.34it/s, train_loss=0.0289]\n",
      "Epoch 3: \n",
      "Losses: train = 0.02872553, test = 0.08485551\n",
      "Test: accuracy = 0.94299915\n",
      "Test: precision = 0.74114947\n",
      "Test: recall = 0.68584473\n",
      "Test: f1_score = 0.70757612\n",
      "new maximum score 0.94299915\n",
      "Saving state_dict... done.\n",
      "Epoch 4: 100%|██████████| 30390/30390 [01:19<00:00, 402.07it/s, train_loss=0.0285]\n",
      "Epoch 4: \n",
      "Losses: train = 0.02818598, test = 0.08503730\n",
      "Test: accuracy = 0.94263959\n",
      "Test: precision = 0.75355072\n",
      "Test: recall = 0.67596584\n",
      "Test: f1_score = 0.70629763\n",
      "BAD EPOCHS: 1 (<< <)\n",
      "Epoch 5: 100%|██████████| 30390/30390 [01:19<00:00, 382.16it/s, train_loss=0.0281]\n",
      "Epoch 5: \n",
      "Losses: train = 0.02822003, test = 0.08459215\n",
      "Test: accuracy = 0.94291455\n",
      "Test: precision = 0.73136577\n",
      "Test: recall = 0.69184012\n",
      "Test: f1_score = 0.70476690\n",
      "BAD EPOCHS: 1 (<< >)\n",
      "Epoch 6: 100%|██████████| 30390/30390 [01:19<00:00, 382.11it/s, train_loss=0.0283]\n",
      "Epoch 6: \n",
      "Losses: train = 0.02850890, test = 0.08504315\n",
      "Test: accuracy = 0.94274535\n",
      "Test: precision = 0.73236490\n",
      "Test: recall = 0.68422821\n",
      "Test: f1_score = 0.70267917\n",
      "BAD EPOCHS: 2 (<< <)\n",
      "Epoch 7: 100%|██████████| 30390/30390 [01:18<00:00, 357.79it/s, train_loss=0.029] \n",
      "Epoch 7: \n",
      "Losses: train = 0.02827262, test = 0.08559027\n",
      "Test: accuracy = 0.94263959\n",
      "Test: precision = 0.76454410\n",
      "Test: recall = 0.67580626\n",
      "Test: f1_score = 0.70906004\n",
      "BAD EPOCHS: 3 (<< <)\n",
      "Epoch 8: 100%|██████████| 30390/30390 [01:19<00:00, 385.94it/s, train_loss=0.028] \n",
      "Epoch 8: \n",
      "Losses: train = 0.02817677, test = 0.08559459\n",
      "Test: accuracy = 0.94263959\n",
      "Test: precision = 0.75323940\n",
      "Test: recall = 0.67621436\n",
      "Test: f1_score = 0.70632887\n",
      "BAD EPOCHS: 4 (<< =)\n",
      "Epoch 9: 100%|██████████| 30390/30390 [01:19<00:00, 400.17it/s, train_loss=0.0281]\n",
      "Epoch 9: \n",
      "Losses: train = 0.02831315, test = 0.08495005\n",
      "Test: accuracy = 0.94270305\n",
      "Test: precision = 0.72979360\n",
      "Test: recall = 0.69445187\n",
      "Test: f1_score = 0.70549770\n",
      "BAD EPOCHS: 4 (<< >)\n",
      "Epoch 10: 100%|██████████| 30390/30390 [01:19<00:00, 381.53it/s, train_loss=0.0288]\n",
      "Epoch 10: \n",
      "Losses: train = 0.02834447, test = 0.08478156\n",
      "Test: accuracy = 0.94272420\n",
      "Test: precision = 0.73003322\n",
      "Test: recall = 0.69346277\n",
      "Test: f1_score = 0.70516147\n",
      "BAD EPOCHS: 4 (<< >)\n",
      "Epoch 11: 100%|██████████| 30390/30390 [01:18<00:00, 385.68it/s, train_loss=0.0287]\n",
      "Epoch 11: \n",
      "Losses: train = 0.02807947, test = 0.08499288\n",
      "Test: accuracy = 0.94272420\n",
      "Test: precision = 0.73043962\n",
      "Test: recall = 0.69180971\n",
      "Test: f1_score = 0.70446554\n",
      "BAD EPOCHS: 5 (<< =)\n",
      "Maximum bad epochs exceeded. Process has been stopped. Best score 0.9429991539763113 (on epoch 3)\n",
      "\n",
      "=== MISC:NE TAGGER TRAINING HAS FINISHED ===\n",
      "\n",
      "Use the `.load('misc-ne_model')` method to start working with the MISC:NE tagger.\n"
     ]
    }
   ],
   "source": [
    "tagger = NeTagger()\n",
    "tagger.load_train_corpus(corpus_train)\n",
    "tagger.load_test_corpus(corpus_dev)\n",
    "\n",
    "_ = tagger.train(MODEL_FN, device='cuda:8', word_emb_type='bert',\n",
    "                 word_emb_path=MODEL_FN.replace('model', BERT_MODEL_FN)\n",
    "                             + '_len512_ep3_bat8_seed42',\n",
    "                 word_emb_tune_params={\n",
    "                     'model_name': BERT_MODEL_FN,\n",
    "                     'max_len': 512, 'epochs': 3, 'batch_size': 8\n",
    "                 },\n",
    "                 rnn_emb_dim=None, cnn_emb_dim=None, cnn_kernels=range(1, 7),\n",
    "                 upos_emb_dim=300, lstm_layers=2, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... done.\n",
      "Creating model... done.\n",
      "Loading state_dict... done.\n",
      "Fit corpus dict... done.\n"
     ]
    }
   ],
   "source": [
    "res_dev = 'corpora/_dev_' + MODEL_FN.replace('_model', '.conllu')\n",
    "res_test = 'corpora/_test_' + MODEL_FN.replace('_model', '.conllu')\n",
    "tagger = NeTagger()\n",
    "tagger.load(MODEL_FN)\n",
    "junky.clear_tqdm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[====] 3799                                                        \n",
      "Corpus has been loaded: 3799 sentences, 47280 tokens\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.predict(corpus_dev, clone_ds=True, save_to=res_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MISC:NE\n",
      "Load corpus\n",
      "[====] 3799                                                        \n",
      "Corpus has been loaded: 3799 sentences, 47280 tokens\n",
      "Processing corpus\n",
      "100%|██████████| 3799/3799 [00:28<00:00, 132.82it/s]\n",
      "MISC:NE total: 17041\n",
      "      correct: 14346\n",
      "        wrong: 2695 [914 excess / 1027 absent / 754 wrong type]\n",
      "     Accuracy: 0.8418520039903762\n",
      "[Total accuracy: 0.9429991539763113]\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MISC:NE\n",
      "Load corpus\n",
      "Load corpus\n",
      "[====] 3799                                                        \n",
      "Corpus has been loaded: 3799 sentences, 47280 tokens\n",
      "[====] 3799                                                        \n",
      "Corpus has been loaded: 3799 sentences, 47280 tokens\n",
      "MISC:NE total: 17041\n",
      "      correct: 14346\n",
      "        wrong: 2695 [914 excess / 1027 absent / 754 wrong type]\n",
      "     Accuracy: 0.8418520039903762\n",
      "[Total accuracy: 0.9429991539763113]\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus_dev, res_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[====] 3798                                                        \n",
      "Corpus has been loaded: 3798 sentences, 47126 tokens\n",
      "Processing corpus\n",
      "100%|██████████| 3798/3798 [00:28<00:00, 133.49it/s]\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.predict(corpus_test, save_to=res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MISC:NE\n",
      "Load corpus\n",
      "[====] 3798                                                        \n",
      "Corpus has been loaded: 3798 sentences, 47126 tokens\n",
      "MISC:NE total: 16722\n",
      "      correct: 14096\n",
      "        wrong: 2626 [1027 excess / 950 absent / 649 wrong type]\n",
      "     Accuracy: 0.8429613682573854\n",
      "[Total accuracy: 0.9442770445189492]\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus_test, clone_ds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MISC:NE\n",
      "Load corpus\n",
      "Load corpus\n",
      "[====] 3798                                                        \n",
      "Corpus has been loaded: 3798 sentences, 47126 tokens\n",
      "[====] 3798                                                        \n",
      "Corpus has been loaded: 3798 sentences, 47126 tokens\n",
      "MISC:NE total: 16722\n",
      "      correct: 14096\n",
      "        wrong: 2626 [1027 excess / 950 absent / 649 wrong type]\n",
      "     Accuracy: 0.8429613682573854\n",
      "[Total accuracy: 0.9442770445189492]\n"
     ]
    }
   ],
   "source": [
    "_ = tagger.evaluate(corpus_test, res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[====] 3798                                                        \n",
      "Corpus has been loaded: 3798 sentences, 47126 tokens\n",
      "Load corpus\n",
      "[====] 3798                                                        \n",
      "Corpus has been loaded: 3798 sentences, 47126 tokens\n",
      "Address: 0.8411097099621689\n",
      "City: 0.0\n",
      "Date: 0.0\n",
      "Department: 0.77129750982962\n",
      "Facility: 0.3817427385892116\n",
      "Geo: 0.8568019093078759\n",
      "Goal: 0.4030054644808743\n",
      "Location: 0.38461538461538464\n",
      "Organization: 0.8547898752573574\n",
      "Person: 0.7269076305220884\n",
      "PersonProperty: 0.7540603248259861\n",
      "Phone: 0.7158469945355191\n",
      "Time: 0.0\n"
     ]
    }
   ],
   "source": [
    "corp_gold = list(tagger._get_corpus(corpus_test, asis=True))\n",
    "corp_test = list(tagger._get_corpus(res_test))\n",
    "tags = sorted(set(x['MISC'].get('NE')\n",
    "                      for x in corp_gold for x in x[0]\n",
    "                          if x['MISC'].get('NE')))\n",
    "for tag in tags:\n",
    "    print('{}: {}'.format(\n",
    "        tag, tagger.evaluate(corp_gold, corp_test,\n",
    "                             label=tag, log_file=None)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
